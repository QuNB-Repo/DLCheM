{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: -1103.222900390625\n"
     ]
    }
   ],
   "source": [
    "import schnetpack as spk\n",
    "import torch\n",
    "import schnetpack.nn \n",
    "import schnetpack.data\n",
    "import pandas as pd\n",
    "import scipy.linalg as la\n",
    "from schnetpack.datasets import QM9\n",
    "\n",
    "# Load best model where it was stored after training\n",
    "\n",
    "best_model = torch.load(\"./trained_models/qm911/best_model\", map_location=torch.device('cpu'))\n",
    "\n",
    "# Download QM9 dataset to use in evaluating model\n",
    "qm9data = QM9('./qm9.db', download=True, remove_uncharacterized=True)\n",
    "\n",
    "# Load split file \n",
    "train, val, test = spk.data.train_test_split(qm9data, split_file='./trained_models/qm911/split.npz')\n",
    "\n",
    "#set up device and atoms converter for input\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "converter = spk.data.AtomsConverter(device=device)\n",
    "\n",
    "test_loader = spk.AtomsLoader(test, batch_size=100)\n",
    "converter = spk.data.AtomsConverter(device=device)\n",
    "\n",
    "at, props = qm9data.get_properties(idx=0)\n",
    "\n",
    "calculator = spk.interfaces.SpkCalculator(model=best_model, device=device, energy=QM9.U0)\n",
    "at.set_calculator(calculator)\n",
    "\n",
    "\n",
    "print('Prediction:', at.get_total_energy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2131, -0.1541,  0.7155,  2.2288, -1.0402,  0.5182, -1.4842,\n",
      "           0.6280,  2.3356, -1.6524,  0.2371,  0.3101, -0.2030, -0.2110,\n",
      "          -0.5656,  1.4761, -0.2728, -0.7652, -0.0495, -1.1851, -0.3630,\n",
      "           0.9301, -1.0113,  0.1016,  1.5456,  0.0256,  1.1504,  0.4426,\n",
      "           0.8667, -0.1562, -0.4621,  0.3061,  0.9261,  0.2881, -1.5597,\n",
      "           1.2288,  0.9443,  1.6847, -1.1722, -1.7441, -0.2525,  0.2136,\n",
      "           1.6990, -1.3576, -0.4959, -0.0181,  0.9218,  0.2518,  0.7842,\n",
      "          -0.1016, -1.0702,  0.1549, -0.3374, -0.3830,  1.2244, -0.0261,\n",
      "          -0.6059,  0.3950, -1.9123, -0.4071,  0.6244,  1.0629, -0.5307,\n",
      "           0.6747, -0.7225,  0.0946,  1.1752, -0.4311,  0.1213,  1.4363,\n",
      "           0.2324, -1.2943, -0.2397,  0.9655,  0.7573, -0.2368,  1.8782,\n",
      "          -0.8502, -1.3292, -0.1816, -0.9571,  0.0477, -0.3671, -0.1935,\n",
      "           2.8333, -0.7572, -0.1475, -1.1664,  0.9843, -0.5542,  0.4030,\n",
      "          -0.2721, -0.3766,  1.0638,  0.2676, -0.4875,  0.2061,  0.4148,\n",
      "           0.7837, -1.1264, -0.3733, -0.8881,  0.3929,  0.7156,  0.0809,\n",
      "          -2.2184,  0.1804,  1.8053, -1.8046, -0.9349, -0.6794, -0.4481,\n",
      "           2.2992, -0.1870,  1.4977,  0.1655,  1.1932, -0.7690,  0.8338,\n",
      "           1.1591, -0.0066, -0.2926, -0.1584,  0.4687,  1.8239,  0.3621,\n",
      "           0.7013,  0.1168],\n",
      "         [-0.0082,  0.3863,  0.9320,  0.2091, -1.0534,  1.1685, -0.1153,\n",
      "           0.0989,  0.1768,  0.3379,  0.6808,  0.3726, -0.9104,  0.6105,\n",
      "          -0.0972,  1.5161, -0.4857,  0.0205,  0.5886, -0.1320,  0.2699,\n",
      "          -1.8473, -0.3639,  0.6684,  0.4155,  2.6073,  0.3915,  0.3873,\n",
      "          -0.9837,  1.4862,  1.0362,  1.8100, -0.2123,  0.9591, -0.6913,\n",
      "          -1.2083,  0.0034, -0.6095,  0.9743, -1.0128,  0.1320, -0.7024,\n",
      "           0.2025, -2.2739, -2.4319, -0.5855,  0.7158, -0.4270,  0.7553,\n",
      "          -0.9148,  0.3843, -0.5613, -0.1539, -0.1062, -0.9392, -0.1510,\n",
      "           0.7673,  0.8608,  1.6080,  0.5315, -0.5743,  0.2378,  0.2772,\n",
      "          -0.1305, -0.2341,  0.1961,  1.2463,  0.3202, -0.6807, -1.0251,\n",
      "           0.2521,  0.1104, -0.1595,  0.3994, -1.9565,  1.2357,  0.2037,\n",
      "           0.2718,  1.7292, -1.1871,  1.2494,  1.6725, -0.4735,  1.8035,\n",
      "           1.8421, -0.5906, -1.0410,  0.1231, -1.2193, -1.1618,  0.2596,\n",
      "           0.4974,  0.6689,  0.3899, -0.1531,  0.6708,  0.0850,  1.7272,\n",
      "           0.2065,  1.0063,  0.0407, -1.1075, -0.1663, -1.6247, -0.5721,\n",
      "          -0.5591, -1.7725, -0.3834, -1.3358, -1.1899,  1.3164,  0.5456,\n",
      "           0.6522,  0.0498,  0.6921,  0.6853,  0.3202, -0.0287, -1.8567,\n",
      "          -1.1335,  1.8558,  1.3463, -0.8528, -1.7218, -0.7034, -1.5617,\n",
      "           1.9850, -1.6186],\n",
      "         [-0.0082,  0.3863,  0.9320,  0.2091, -1.0534,  1.1685, -0.1153,\n",
      "           0.0989,  0.1768,  0.3379,  0.6808,  0.3726, -0.9104,  0.6105,\n",
      "          -0.0972,  1.5161, -0.4857,  0.0205,  0.5886, -0.1320,  0.2699,\n",
      "          -1.8473, -0.3639,  0.6684,  0.4155,  2.6073,  0.3915,  0.3873,\n",
      "          -0.9837,  1.4862,  1.0362,  1.8100, -0.2123,  0.9591, -0.6913,\n",
      "          -1.2083,  0.0034, -0.6095,  0.9743, -1.0128,  0.1320, -0.7024,\n",
      "           0.2025, -2.2739, -2.4319, -0.5855,  0.7158, -0.4270,  0.7553,\n",
      "          -0.9148,  0.3843, -0.5613, -0.1539, -0.1062, -0.9392, -0.1510,\n",
      "           0.7673,  0.8608,  1.6080,  0.5315, -0.5743,  0.2378,  0.2772,\n",
      "          -0.1305, -0.2341,  0.1961,  1.2463,  0.3202, -0.6807, -1.0251,\n",
      "           0.2521,  0.1104, -0.1595,  0.3994, -1.9565,  1.2357,  0.2037,\n",
      "           0.2718,  1.7292, -1.1871,  1.2494,  1.6725, -0.4735,  1.8035,\n",
      "           1.8421, -0.5906, -1.0410,  0.1231, -1.2193, -1.1618,  0.2596,\n",
      "           0.4974,  0.6689,  0.3899, -0.1531,  0.6708,  0.0850,  1.7272,\n",
      "           0.2065,  1.0063,  0.0407, -1.1075, -0.1663, -1.6247, -0.5721,\n",
      "          -0.5591, -1.7725, -0.3834, -1.3358, -1.1899,  1.3164,  0.5456,\n",
      "           0.6522,  0.0498,  0.6921,  0.6853,  0.3202, -0.0287, -1.8567,\n",
      "          -1.1335,  1.8558,  1.3463, -0.8528, -1.7218, -0.7034, -1.5617,\n",
      "           1.9850, -1.6186],\n",
      "         [-0.0082,  0.3863,  0.9320,  0.2091, -1.0534,  1.1685, -0.1153,\n",
      "           0.0989,  0.1768,  0.3379,  0.6808,  0.3726, -0.9104,  0.6105,\n",
      "          -0.0972,  1.5161, -0.4857,  0.0205,  0.5886, -0.1320,  0.2699,\n",
      "          -1.8473, -0.3639,  0.6684,  0.4155,  2.6073,  0.3915,  0.3873,\n",
      "          -0.9837,  1.4862,  1.0362,  1.8100, -0.2123,  0.9591, -0.6913,\n",
      "          -1.2083,  0.0034, -0.6095,  0.9743, -1.0128,  0.1320, -0.7024,\n",
      "           0.2025, -2.2739, -2.4319, -0.5855,  0.7158, -0.4270,  0.7553,\n",
      "          -0.9148,  0.3843, -0.5613, -0.1539, -0.1062, -0.9392, -0.1510,\n",
      "           0.7673,  0.8608,  1.6080,  0.5315, -0.5743,  0.2378,  0.2772,\n",
      "          -0.1305, -0.2341,  0.1961,  1.2463,  0.3202, -0.6807, -1.0251,\n",
      "           0.2521,  0.1104, -0.1595,  0.3994, -1.9565,  1.2357,  0.2037,\n",
      "           0.2718,  1.7292, -1.1871,  1.2494,  1.6725, -0.4735,  1.8035,\n",
      "           1.8421, -0.5906, -1.0410,  0.1231, -1.2193, -1.1618,  0.2596,\n",
      "           0.4974,  0.6689,  0.3899, -0.1531,  0.6708,  0.0850,  1.7272,\n",
      "           0.2065,  1.0063,  0.0407, -1.1075, -0.1663, -1.6247, -0.5721,\n",
      "          -0.5591, -1.7725, -0.3834, -1.3358, -1.1899,  1.3164,  0.5456,\n",
      "           0.6522,  0.0498,  0.6921,  0.6853,  0.3202, -0.0287, -1.8567,\n",
      "          -1.1335,  1.8558,  1.3463, -0.8528, -1.7218, -0.7034, -1.5617,\n",
      "           1.9850, -1.6186],\n",
      "         [-0.0082,  0.3863,  0.9320,  0.2091, -1.0534,  1.1685, -0.1153,\n",
      "           0.0989,  0.1768,  0.3379,  0.6808,  0.3726, -0.9104,  0.6105,\n",
      "          -0.0972,  1.5161, -0.4857,  0.0205,  0.5886, -0.1320,  0.2699,\n",
      "          -1.8473, -0.3639,  0.6684,  0.4155,  2.6073,  0.3915,  0.3873,\n",
      "          -0.9837,  1.4862,  1.0362,  1.8100, -0.2123,  0.9591, -0.6913,\n",
      "          -1.2083,  0.0034, -0.6095,  0.9743, -1.0128,  0.1320, -0.7024,\n",
      "           0.2025, -2.2739, -2.4319, -0.5855,  0.7158, -0.4270,  0.7553,\n",
      "          -0.9148,  0.3843, -0.5613, -0.1539, -0.1062, -0.9392, -0.1510,\n",
      "           0.7673,  0.8608,  1.6080,  0.5315, -0.5743,  0.2378,  0.2772,\n",
      "          -0.1305, -0.2341,  0.1961,  1.2463,  0.3202, -0.6807, -1.0251,\n",
      "           0.2521,  0.1104, -0.1595,  0.3994, -1.9565,  1.2357,  0.2037,\n",
      "           0.2718,  1.7292, -1.1871,  1.2494,  1.6725, -0.4735,  1.8035,\n",
      "           1.8421, -0.5906, -1.0410,  0.1231, -1.2193, -1.1618,  0.2596,\n",
      "           0.4974,  0.6689,  0.3899, -0.1531,  0.6708,  0.0850,  1.7272,\n",
      "           0.2065,  1.0063,  0.0407, -1.1075, -0.1663, -1.6247, -0.5721,\n",
      "          -0.5591, -1.7725, -0.3834, -1.3358, -1.1899,  1.3164,  0.5456,\n",
      "           0.6522,  0.0498,  0.6921,  0.6853,  0.3202, -0.0287, -1.8567,\n",
      "          -1.1335,  1.8558,  1.3463, -0.8528, -1.7218, -0.7034, -1.5617,\n",
      "           1.9850, -1.6186]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Extract embedding from schnetpack\n",
    "from schnetpack.representation import SchNet\n",
    "\n",
    "model = SchNet()\n",
    "\n",
    "embedding_output=None\n",
    "\n",
    "def embedding_hook(self, inp_tensor, out_tensor):\n",
    "   # Global allows us to utilize embedding_output outside the current function scope\n",
    "   global embedding_output \n",
    "   embedding_output=out_tensor\n",
    "\n",
    "model.embedding.register_forward_hook(embedding_hook)\n",
    "\n",
    "inputs = converter(at)\n",
    "model(inputs)\n",
    "\n",
    "print(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### ANOTHER WAY OF DOING IT, given in issues\n",
    "\n",
    "activation = {}\n",
    "\n",
    "inputs = converter(at)\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(module, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model = SchNet()\n",
    "model.embedding.register_forward_hook(get_activation('Embedding'))\n",
    "output = model(inputs)\n",
    "\n",
    "print(activation['Embedding'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
