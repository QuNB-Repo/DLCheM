{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of SMILES2VEC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some tools that will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "'''\n",
    "This code defines and trains the SMILES2Vec on a dataset that has \n",
    "1) SMILES 2) molecule property 3) train/test split value in columns of a csv file\n",
    "\n",
    "First we will define some important tools that are used by the MAIN code\n",
    "\n",
    "Variables:\n",
    "\n",
    "            vectorize           - A function that converts SMILES character set to a one-hot embedding\n",
    "            one_hot             - this is a tensor that is #data x #embed_size x #characters,\n",
    "                                  each SMILES string will have an associated embedding of size (max(smiles) + 50)\n",
    "                                  each embedding will have a vector of size #characters, where one of them will be\n",
    "                                  \"1\" and the others \"0\" depending on which character is in the SMILES. The first\n",
    "                                  character is always \"!\" which does have an integer associated which is the \"1\" in \n",
    "                                  first embedding vector. All the final characters of the string are \"E\" which also\n",
    "                                  have an integer associated with them and will have embeddings with an element as \"1\".\n",
    "                                  The rest of the characters are all in between, each with their own embedding, and \n",
    "                                  and an element of \"1\" lighting up for the integer associated with that character\n",
    "            R2_calc             - a tool that calculates the R^2 between predicted and true values\n",
    "\n",
    "'''\n",
    "#Vectorize the SMILES strings in one-hot embeddings of size of the\n",
    "#embedding (len largest SMILES string + 5) and charset (number of unique characters in the SMILES data). \n",
    "#One-hot encoding will just light up the character that each part of the SMILES corresponds to (56x27),\n",
    "#for example the 3rd character might be a '[' which means that the third row will have a \"1\" in the 27th column\n",
    "#... etc \n",
    "def vectorize(smiles,max_stringsize,charset,char_to_int):\n",
    "        \n",
    "        #initialize the vectorized data as the #data, embed_size, len(charset)\n",
    "        one_hot =  np.zeros((smiles.shape[0], max_stringsize , len(charset)),dtype=np.int8)\n",
    "\n",
    "        #for each molecule and associated smiles string in data \n",
    "        for each_molecule, smile in enumerate(smiles):\n",
    "            #encode the startchar, the first character in all smiles is \"!\" and \n",
    "            #this will take one position in the embedding vector for this character\n",
    "            one_hot[each_molecule,0,char_to_int[\"!\"]] = 1\n",
    "            #encode the rest of the chars, depending on what their integer associated with the character\n",
    "            for j,c in enumerate(smile):\n",
    "                one_hot[each_molecule,j+1,char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[each_molecule,len(smile)+1:,char_to_int[\"E\"]] = 1\n",
    "        return one_hot\n",
    "\n",
    "def r2_calc(y_true, y_pred):\n",
    "    SS_res = torch.sum((y_true - y_pred)**2)\n",
    "    SS_tot = torch.sum((y_true - torch.mean(y_true))**2)\n",
    "    epsilon = torch.finfo(y_true.dtype).eps  # Small constant to avoid division by zero\n",
    "    return 1 - SS_res / (SS_tot + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1434, 102, 27])\n",
      "torch.Size([1434, 1])\n",
      "torch.Size([358, 102, 27])\n",
      "torch.Size([358, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "This code defines and trains the SMILES2Vec on a dataset that has \n",
    "1) SMILES 2) molecule property 3) train/test split value in columns of a csv file\n",
    "\n",
    "Variables: \n",
    "\n",
    "        data             - loaded from a csv file that contains the properties mentioned above\n",
    "        X_train_smiles   - the SMILES train set extracted using the split column\n",
    "        X_test_smiles    - the SMILES test set extracted using the split column\n",
    "        property         - the target property for training\n",
    "        Y_train          - the train targets extracted using the split column\n",
    "        Y_test           - the test targets extracted using the split column\n",
    "        charset          - the set of unique SMILES characters in the data, extracted by using \"set\" on \n",
    "                           list joining all SMILES, \"\".join(list(data.smiles))\n",
    "        char_to_int      - dictionary that maps char to integers, dict((c,i) for i,c in enumerate(charset))\n",
    "        int_to_char      - dictionary that maps integers to chars, dict((i,c) for i,c in enumerate(charset))\n",
    "        embed _size      - the size of the embedding which is taken to be the size of the LARGEST SMILES + 50\n",
    "        X_train_embed    - the embedded training SMILES retrieved using the vectorize tool  \n",
    "        X_test_embed     - the embedded testing SMILES retrieved using the vectorize tool\n",
    "        X_train_tensor   - the tensor verion of X_train_embed\n",
    "        X_test_tensor    - the tensor version of X_test_embed\n",
    "        mol_str_train    - the SMILES train set which is now embedded and has a starting \"!\" and\n",
    "                           \"E\" for all the ending charachters\n",
    "        mol_str_test     - the SMILES test set which is now embedded and has a starting \"!\" and\n",
    "                           \"E\" for all the ending charachters\n",
    "'''\n",
    "\n",
    "#LOAD the SMILESvsProperties data\n",
    "data = pd.read_csv('data/IGC50.csv')\n",
    "\n",
    "#Split the data into training and testing\n",
    "X_train_smiles = np.array(list(data[\"smiles\"][data[\"split\"]==1]))\n",
    "X_test_smiles = np.array(list(data[\"smiles\"][data[\"split\"]==0]))\n",
    "\n",
    "\n",
    "property = \"Activity\"  \n",
    "Y_train = data[property][data[\"split\"]==1].values.reshape(-1,1)\n",
    "Y_test = data[property][data[\"split\"]==0].values.reshape(-1,1)\n",
    "\n",
    "#Get the smiles vocabulary from the SMILES dataset\n",
    "# charset --> the set of characters in the SMILES\n",
    "# char_to_int --> dictionary that maps characters to integers\n",
    "# int_to_char --> dictionary that maps integers back to characters\n",
    "# embed --> the length of the longest SMILES string in the dataset + 5\n",
    "charset = set(\"\".join(list(data.smiles))+\"!E\")\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "max_stringsize = max([len(smile) for smile in data.smiles]) + 50\n",
    "\n",
    "X_train_embed = vectorize(X_train_smiles,max_stringsize,charset,char_to_int)\n",
    "X_test_embed = vectorize(X_test_smiles,max_stringsize,charset,char_to_int)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_embed).long()  # Assuming X_train is integer-based data\n",
    "Y_train_tensor = torch.from_numpy(Y_train).float()  # Assuming Y_train is float-based data\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test_embed).long()  # Assuming X_test is integer-based data\n",
    "Y_test_tensor = torch.from_numpy(Y_test).float()  # Assuming Y_test is float-based data\n",
    "\n",
    "#CONVERT embeddings (hot encodings) back to SMILES\n",
    "mol_str_train=[]\n",
    "mol_str_test=[]\n",
    "for x in range(X_train_embed.shape[0]):\n",
    "    mol_str_train.append(\"\".join([int_to_char[idx] for idx in np.argmax(X_train_embed[x,:,:], axis=1)]))\n",
    "    \n",
    "for x in range(len(charset)):\n",
    "    mol_str_test.append(\"\".join([int_to_char[idx] for idx in np.argmax(X_test_embed[x,:,:], axis=1)]))\n",
    "vocab_size=len(charset)\n",
    "\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "print(Y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434\n",
      "epoch 0\n",
      "train_loss 7.086065298343802\n",
      "val_loss 2.8373842492449883\n",
      "1434\n",
      "epoch 1\n",
      "train_loss 1.353079978392214\n",
      "val_loss 1.3359108397414565\n",
      "1434\n",
      "epoch 2\n",
      "train_loss 1.4200400503110686\n",
      "val_loss 0.9782402222382955\n",
      "1434\n",
      "epoch 3\n",
      "train_loss 1.174914565711507\n",
      "val_loss 0.9735858853302854\n",
      "1434\n",
      "epoch 4\n",
      "train_loss 1.1347138918260817\n",
      "val_loss 0.9670308195678882\n",
      "1434\n",
      "epoch 5\n",
      "train_loss 1.1658954773321644\n",
      "val_loss 0.9661069499047775\n",
      "1434\n",
      "epoch 6\n",
      "train_loss 1.1492688690768127\n",
      "val_loss 0.9646739310392455\n",
      "1434\n",
      "epoch 7\n",
      "train_loss 1.155073072454753\n",
      "val_loss 0.964707285665267\n",
      "1434\n",
      "epoch 8\n",
      "train_loss 1.1556867786697431\n",
      "val_loss 0.9647281246478331\n",
      "1434\n",
      "epoch 9\n",
      "train_loss 1.1555895173566277\n",
      "val_loss 0.9646690351337028\n",
      "1434\n",
      "epoch 10\n",
      "train_loss 1.1570163090691241\n",
      "val_loss 0.9646746702700354\n",
      "1434\n",
      "epoch 11\n",
      "train_loss 1.1572257172279943\n",
      "val_loss 0.9646586486081171\n",
      "1434\n",
      "epoch 12\n",
      "train_loss 1.1579608336984695\n",
      "val_loss 0.9646552235054571\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.\n",
      "1434\n",
      "epoch 13\n",
      "train_loss 1.1497740220159\n",
      "val_loss 0.9650940112561487\n",
      "1434\n",
      "epoch 14\n",
      "train_loss 1.1482530275315586\n",
      "val_loss 0.9655800498397656\n",
      "1434\n",
      "epoch 15\n",
      "train_loss 1.1463227750865985\n",
      "val_loss 0.9651765643551363\n",
      "1434\n",
      "epoch 16\n",
      "train_loss 1.147511852336229\n",
      "val_loss 0.9651734795650291\n",
      "1434\n",
      "epoch 17\n",
      "train_loss 1.147816970923622\n",
      "val_loss 0.9652199868383354\n",
      "1434\n",
      "epoch 18\n",
      "train_loss 1.1477462049973726\n",
      "val_loss 0.9651764881011494\n",
      "Epoch 00019: reducing learning rate of group 0 to 2.5000e-05.\n",
      "1434\n",
      "epoch 19\n",
      "train_loss 1.1436319710320508\n",
      "val_loss 0.9654342818526582\n",
      "1434\n",
      "epoch 20\n",
      "train_loss 1.1436354896180965\n",
      "val_loss 0.965832223106363\n",
      "1434\n",
      "epoch 21\n",
      "train_loss 1.142918707935381\n",
      "val_loss 0.9658430135449884\n",
      "1434\n",
      "epoch 22\n",
      "train_loss 1.1428170270667082\n",
      "val_loss 0.9657446205948984\n",
      "1434\n",
      "epoch 23\n",
      "train_loss 1.1430175011793107\n",
      "val_loss 0.9657147220393133\n",
      "1434\n",
      "epoch 24\n",
      "train_loss 1.143145450488294\n",
      "val_loss 0.9657203541787643\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2500e-05.\n",
      "1434\n",
      "epoch 25\n",
      "train_loss 1.1409493965750126\n",
      "val_loss 0.9658172300408007\n",
      "1434\n",
      "epoch 26\n",
      "train_loss 1.1410213663987037\n",
      "val_loss 0.9659927936905589\n",
      "1434\n",
      "epoch 27\n",
      "train_loss 1.140857638674301\n",
      "val_loss 0.9660826768289065\n",
      "1434\n",
      "epoch 28\n",
      "train_loss 1.1407558462443546\n",
      "val_loss 0.966097181735758\n",
      "1434\n",
      "epoch 29\n",
      "train_loss 1.1407417078896047\n",
      "val_loss 0.9660843783916708\n",
      "1434\n",
      "epoch 30\n",
      "train_loss 1.1407696643800083\n",
      "val_loss 0.9660712140898465\n",
      "Epoch 00031: reducing learning rate of group 0 to 6.2500e-06.\n",
      "1434\n",
      "epoch 31\n",
      "train_loss 1.1396494284168281\n",
      "val_loss 0.9660973419024291\n",
      "1434\n",
      "epoch 32\n",
      "train_loss 1.1396896760999076\n",
      "val_loss 0.9661579828022578\n",
      "1434\n",
      "epoch 33\n",
      "train_loss 1.1396619420670067\n",
      "val_loss 0.9662063567998023\n",
      "1434\n",
      "epoch 34\n",
      "train_loss 1.1396327755274966\n",
      "val_loss 0.9662357998293871\n",
      "1434\n",
      "epoch 35\n",
      "train_loss 1.1396162667866365\n",
      "val_loss 0.9662509520626601\n",
      "1434\n",
      "epoch 36\n",
      "train_loss 1.1396111304789907\n",
      "val_loss 0.9662575718410854\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.1250e-06.\n",
      "1434\n",
      "epoch 37\n",
      "train_loss 1.1390123731421626\n",
      "val_loss 0.966267255431447\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class smiles2vec(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size, hidden_size1, hidden_size2):\n",
    "        super(smiles2vec, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.gru1 = nn.GRU(input_size=embed_size, hidden_size=hidden_size1//2, num_layers=2, batch_first=True, dtype=torch.float32, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(input_size=hidden_size1, hidden_size=hidden_size2//2, num_layers=2, batch_first=True, dtype=torch.float32, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h1 = torch.zeros(4, x.size(0), hidden_size1//2).to(x.device)\n",
    "        x, _ = self.gru1(x, h1)\n",
    "        h2 = torch.zeros(4, x.size(0), hidden_size2//2).to(x.device)\n",
    "        x, _ = self.gru2(x, h2)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#Define hyperparameters\n",
    "vocab_size = vocab_size\n",
    "embed_size = 50\n",
    "hidden_size1 = 112*2\n",
    "hidden_size2 = 192*2\n",
    "\n",
    "# Define your PyTorch model\n",
    "model = smiles2vec(vocab_size,embed_size,hidden_size1,hidden_size2)  # Assuming 'vocab_size' and 'embed' are defined\n",
    "\n",
    "# Define your optimizer and learning rate\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define your learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-15, verbose=True)\n",
    "\n",
    "# Define your loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Assuming X_train, Y_train, X_test, Y_test are PyTorch tensors or converted to PyTorch tensors\n",
    "\n",
    "# Create DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(torch.argmax(X_train_tensor, dim=2), Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(torch.argmax(X_test_tensor, dim=2), Y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20)\n",
    "\n",
    "# Training loop\n",
    "trainloss_profile = []  # For logging\n",
    "valloss_profile = []\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "#        train_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    print(len(train_loader.dataset))\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "#            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print('epoch',epoch)\n",
    "    print('train_loss',train_loss)\n",
    "    print('val_loss',val_loss)\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Logging\n",
    "    trainloss_profile.append([epoch,train_loss])\n",
    "    valloss_profile.append([epoch,val_loss])\n",
    "    \n",
    "    # Checkpointing\n",
    "#    if val_loss < best_val_loss:  # Save best model based on validation loss\n",
    "#        best_val_loss = val_loss\n",
    "#        torch.save(model.state_dict(), 'weights_best.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
