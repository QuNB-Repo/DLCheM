{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of SMILES2VEC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data of SMILES strings with their properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading & Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "'''\n",
    "This script processes molecular data in the form of SMILES strings and prepares it for machine learning models.\n",
    "The data is loaded from a CSV file and split into training and testing sets based on a \"split\" column.\n",
    "The SMILES strings are one-hot encoded using a custom function that includes special start ('!') and end ('E')\n",
    "characters, ensuring all sequences have a uniform length. The encoded SMILES strings are converted into PyTorch \n",
    "tensors, making them suitable for training and testing a deep learning model.\n",
    "\n",
    "The script also handles the assay activity values, which are reshaped and converted into PyTorch tensors as well. \n",
    "Additionally, after vectorization, the SMILES strings are decoded back into their original form for verification.\n",
    "\n",
    "Key steps:\n",
    "1. Load the SMILES and activity data from a CSV file and split into training and testing sets.\n",
    "2. Create a character set from all unique characters found in SMILES and map characters to integers and vice versa.\n",
    "3. Vectorize (one-hot encode) the SMILES strings with start, end, and padding characters.\n",
    "4. Convert the one-hot encoded data and activity values into PyTorch tensors for use in a deep learning model.\n",
    "5. Decode the one-hot encoded SMILES back to their original form for verification.\n",
    "'''\n",
    "\n",
    "\n",
    "# Load the data from a CSV file containing SMILES strings and assay activity\n",
    "DATA = pd.read_csv('data/IGC50.csv')\n",
    "\n",
    "\n",
    "# Split the SMILES data into training and testing sets based on the \"split\" column in the data\n",
    "X_train_smiles = np.array(list(DATA[\"smiles\"][DATA[\"split\"] == 1]))  # Training SMILES strings\n",
    "X_test_smiles = np.array(list(DATA[\"smiles\"][DATA[\"split\"] == 0]))   # Testing SMILES strings\n",
    "\n",
    "# Extract the assay values (e.g., activity) for training and testing sets based on the \"split\" column\n",
    "assay = \"Activity\"  # Column name for the assay values\n",
    "Y_train = DATA[assay][DATA[\"split\"] == 1].values.reshape(-1, 1)  # Training labels (assay values)\n",
    "Y_test = DATA[assay][DATA[\"split\"] == 0].values.reshape(-1, 1)   # Testing labels (assay values)\n",
    "\n",
    "# Print the shapes of the training and testing data\n",
    "print(X_train_smiles.shape, Y_train.shape)\n",
    "print(X_test_smiles.shape, Y_test.shape)\n",
    "\n",
    "# Create a character set based on all unique characters in the SMILES strings plus special start and end chars\n",
    "charset = set(\"\".join(list(DATA.smiles)) + \"!E\")\n",
    "\n",
    "# Create mappings between characters and integers (for encoding and decoding)\n",
    "char_to_int = dict((c, i) for i, c in enumerate(charset))  # Map characters to integers\n",
    "int_to_char = dict((i, c) for i, c in enumerate(charset))  # Map integers to characters\n",
    "\n",
    "# Calculate the maximum length of a SMILES string and add padding to ensure uniform length\n",
    "embed = max([len(smile) for smile in DATA.smiles]) + 50  # Add padding space of 50\n",
    "\n",
    "# Print the charset and embedding size (vocabulary size and maximum sequence length)\n",
    "print(str(charset))\n",
    "print(len(charset), embed)\n",
    "\n",
    "# Show the character-to-integer mapping\n",
    "char_to_int\n",
    "\n",
    "# Define a function to vectorize the SMILES strings into one-hot encoded arrays\n",
    "def vectorize(smiles):\n",
    "    # Initialize a one-hot encoded array of shape (number of smiles, max length, number of unique characters)\n",
    "    one_hot = np.zeros((smiles.shape[0], embed, len(charset)), dtype=np.int8)\n",
    "    \n",
    "    # Loop over each SMILES string and encode the characters\n",
    "    for i, smile in enumerate(smiles):\n",
    "        # Encode the start character '!'\n",
    "        one_hot[i, 0, char_to_int[\"!\"]] = 1\n",
    "        \n",
    "        # Encode each character in the SMILES string\n",
    "        for j, c in enumerate(smile):\n",
    "            one_hot[i, j + 1, char_to_int[c]] = 1  # Offset by 1 to account for start character\n",
    "        \n",
    "        # Encode the end character 'E'\n",
    "        one_hot[i, len(smile) + 1:, char_to_int[\"E\"]] = 1\n",
    "    \n",
    "    # Return the one-hot encoded SMILES for input and output (shifted by one position)\n",
    "    return one_hot[:, 0:-1, :], one_hot[:, 1:, :]\n",
    "\n",
    "# Vectorize the SMILES strings for training and testing sets\n",
    "X_train, _ = vectorize(X_train_smiles)\n",
    "X_test, _ = vectorize(X_test_smiles)\n",
    "\n",
    "# Print the shape of the vectorized training set (for confirmation)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors for training and evaluation\n",
    "X_train_tensor = torch.from_numpy(X_train).long()  # SMILES strings (one-hot encoded)\n",
    "Y_train_tensor = torch.from_numpy(Y_train).float()  # Assay activity values (float-based)\n",
    "\n",
    "# Print the shape of the assay activity values\n",
    "print(Y_train.shape)\n",
    "\n",
    "# Convert the test data to PyTorch tensors\n",
    "X_test_tensor = torch.from_numpy(X_test).long()  # SMILES strings (one-hot encoded)\n",
    "Y_test_tensor = torch.from_numpy(Y_test).float()  # Assay activity values (float-based)\n",
    "\n",
    "# Initialize lists to store decoded SMILES strings from the one-hot encoded vectors for training and testing sets\n",
    "mol_str_train = []\n",
    "mol_str_test = []\n",
    "\n",
    "# Decode the one-hot encoded training SMILES strings back to their original form\n",
    "for x in range(1434):  # Number of training samples\n",
    "    mol_str_train.append(\"\".join([int_to_char[idx] for idx in np.argmax(X_train[x, :, :], axis=1)]))\n",
    "\n",
    "# Decode the one-hot encoded testing SMILES strings back to their original form\n",
    "for x in range(358):  # Number of testing samples\n",
    "    mol_str_test.append(\"\".join([int_to_char[idx] for idx in np.argmax(X_test[x, :, :], axis=1)]))\n",
    "\n",
    "# Get the size of the vocabulary (number of unique characters)\n",
    "vocab_size = len(charset)\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Pytorch CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "This code defines a PyTorch neural network architecture called `CNNonSMILES`, designed for processing SMILES (Simplified Molecular \n",
    "Input Line Entry System) strings and predicting molecular properties. The architecture consists of an embedding layer for \n",
    "vectorizing SMILES characters, a GRU layer for capturing sequential relationships, convolutional layers for feature extraction, \n",
    "and fully connected layers for final prediction.\n",
    "\n",
    "The model architecture includes the following key components:\n",
    "1. **Embedding layer**: Converts SMILES characters into dense vectors.\n",
    "2. **GRU (Gated Recurrent Unit)**: A recurrent layer to handle the sequential nature of SMILES data.\n",
    "3. **Convolutional layers**: Three 1D convolutional layers (with BatchNorm applied after the first one) to extract spatial \n",
    "   features from the sequences.\n",
    "4. **Fully connected layers**: Dense layers to process the flattened features extracted by the convolutional layers and \n",
    "   output the final prediction (e.g., molecular property).\n",
    "5. **Dropout**: A dropout layer to prevent overfitting by randomly zeroing some of the layer’s outputs during training.\n",
    "\n",
    "The forward method returns the model’s output prediction as well as intermediate outputs from various layers (embedding, \n",
    "first convolutional layer, final convolutional layer).\n",
    "\n",
    "This architecture is suitable for tasks such as molecular property prediction from SMILES strings.\n",
    "\n",
    "'''\n",
    "\n",
    "# Define the CNNonSMILES neural network architecture\n",
    "class CNNonSMILES(nn.Module):\n",
    "    def __init__(self, vocab_size, embed):\n",
    "        super(CNNonSMILES, self).__init__()\n",
    "        \n",
    "        # Embedding layer: Converts input SMILES characters into dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, 50, padding_idx=0)\n",
    "        \n",
    "        # GRU layer: Captures sequential dependencies in the SMILES strings\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=50, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # Convolutional layers (1D):\n",
    "        # First convolutional layer with BatchNorm\n",
    "        self.conv1 = nn.Conv1d(50, 192, kernel_size=10)\n",
    "        self.bn1 = nn.BatchNorm1d(192)  # Apply batch normalization after the first convolution\n",
    "        # Second and third convolutional layers for further feature extraction\n",
    "        self.conv2 = nn.Conv1d(192, 192, kernel_size=5)\n",
    "        self.conv3 = nn.Conv1d(192, 192, kernel_size=3)\n",
    "        \n",
    "        # Flatten layer: Converts the multidimensional output of the convolutional layers into a 1D vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected (dense) layers:\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(16512, 100)\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.dropout = nn.Dropout(0.4)  # Dropout to prevent overfitting\n",
    "        \n",
    "        # Second fully connected layer (final output)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        x_emb = x  # Store the embedding output\n",
    "        \n",
    "        # Permute the tensor to match the input format expected by Conv1D (batch, channels, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through the first convolutional layer and apply batch normalization and ReLU activation\n",
    "        x = self.conv1(x)\n",
    "        x_conv1 = x  # Store the output after the first convolution\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Pass through the second and third convolutional layers with ReLU activations\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x_final = x  # Store the output after the final convolutional layer\n",
    "        \n",
    "        # Flatten the output from the convolutional layers\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Pass through the first fully connected layer and apply ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Apply dropout to prevent overfitting\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final output prediction through the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Return the final prediction, and intermediate outputs for visualization or analysis\n",
    "        return x, x_emb, x_conv1, x_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script defines a training and validation pipeline for a PyTorch model (assumed to be `CNNonSMILES` or a custom model). \n",
    "It includes:\n",
    "1. **Model Definition**: The model is defined with parameters like `vocab_size` and `embed`.\n",
    "2. **Optimizer**: The Adam optimizer is used to update the model weights.\n",
    "3. **Learning Rate Scheduler**: A scheduler (`ReduceLROnPlateau`) is used to reduce the learning rate when validation loss \n",
    "   plateaus.\n",
    "4. **Loss Function**: Mean Squared Error (MSE) loss is used to compute the difference between the predicted and actual values.\n",
    "5. **DataLoader**: DataLoaders for training and validation data are created, with batch sizes for the respective datasets.\n",
    "6. **Training Loop**: A loop runs for `num_epochs` iterations, where the model trains on the data and evaluates performance \n",
    "   on a validation set. Training and validation losses are tracked.\n",
    "7. **Checkpointing**: The model's best weights (based on validation loss) are saved.\n",
    "8. **Learning Rate Adjustment**: The scheduler adjusts the learning rate based on validation loss improvements.\n",
    "\n",
    "This pipeline handles SMILES data and is structured to train the model, evaluate it, and dynamically adjust learning \n",
    "parameters during training.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define your PyTorch model\n",
    "model = CNNonSMILES(vocab_size, embed)  # Assuming 'vocab_size' and 'embed' are defined for the SMILES model\n",
    "\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define a learning rate scheduler to reduce the LR when the validation loss plateaus\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-15, verbose=True)\n",
    "\n",
    "# Define the loss function (Mean Squared Error for regression tasks)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Create DataLoader for training and validation data\n",
    "# The SMILES tensors are passed through torch.argmax to reduce dimensionality (from one-hot encoding) for training\n",
    "train_dataset = TensorDataset(torch.argmax(X_train_tensor, dim=2), Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1434, shuffle=False)  # Load training data in batches\n",
    "\n",
    "val_dataset = TensorDataset(torch.argmax(X_test_tensor, dim=2), Y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=358)  # Load validation data\n",
    "\n",
    "# Initialize lists to log the loss during training and validation\n",
    "trainloss_profile = []\n",
    "valloss_profile = []\n",
    "num_epochs = 150  # Set the number of training epochs\n",
    "\n",
    "# Training loop: Loop over the dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0  # Reset training loss for the epoch\n",
    "    \n",
    "    # Loop over the training data batches\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs, x_emb_train, x_conv1_train, x_final_train = model(inputs)  # Forward pass through the model\n",
    "        loss = loss_function(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        train_loss += loss.item() * inputs.size(0)  # Accumulate training loss\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)  # Compute the average loss for the training set\n",
    "    \n",
    "    # Validation phase (without gradient updates)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Reset validation loss for the epoch\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations during validation\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs, x_emb_val, x_conv1_val, x_final_val = model(inputs)  # Forward pass on validation data\n",
    "            loss = loss_function(outputs, targets)  # Compute loss\n",
    "            val_loss += loss.item() * inputs.size(0)  # Accumulate validation loss\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)  # Compute the average loss for the validation set\n",
    "    \n",
    "    # Print the current epoch number and the corresponding losses\n",
    "    print('epoch', epoch)\n",
    "    print('train_loss', train_loss)\n",
    "    print('val_loss', val_loss)\n",
    "\n",
    "    # Step the learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Log the losses for this epoch\n",
    "    trainloss_profile.append([epoch, train_loss])\n",
    "    valloss_profile.append([epoch, val_loss])\n",
    "    \n",
    "    # Checkpointing: Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:  # If current validation loss is the best so far, save the model\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'weights_best.pth')  # Save the model's state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract latent vector space representation  (embeddings) of the data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part of the code extracts specific latent vectors or embeddings (convolutional outputs) of the model. The goal is to:\n",
    "1. **Combine Training and Validation Embeddings**: Stack the convolutional outputs from both the training and validation \n",
    "   data into one array (`x_conv1_all`) for analysis.\n",
    "2. **Process Inputs**: The input SMILES data, reduced from one-hot encoding to integer indices, is also combined for \n",
    "   training and validation sets (`X_inp_all`).\n",
    "3. **Target Vocabulary Extraction**: The code loops through each molecule and character in the SMILES strings, identifies \n",
    "   instances of a specific character (vocabulary element), and retrieves the corresponding convolutional layer outputs for \n",
    "   that character (latent vectors).\n",
    "4. **Save Embeddings**: For each instance where the target vocabulary element is found, its latent vector is concatenated \n",
    "   with the molecule index and saved to a CSV file (`conv1embs.csv`).\n",
    "\n",
    "The embeddings or latent vectors represent how specific characters in the SMILES strings are transformed during the \n",
    "convolutional operations.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Stack (combine) the convolutional layer outputs from both the training and validation sets\n",
    "x_conv1_all = np.vstack((x_conv1_train.detach().numpy(), x_conv1_val.detach().numpy()))\n",
    "\n",
    "# Stack (combine) the input SMILES data (after torch.argmax reduces one-hot encoding to integer indices)\n",
    "X_inp_all = np.vstack((torch.argmax(X_train_tensor, dim=2), torch.argmax(X_test_tensor, dim=2)))\n",
    "\n",
    "# Print the shapes of the input SMILES data and the convolutional layer outputs\n",
    "print(X_inp_all.shape)  # Shape should be (number of molecules, max_sequence_length)\n",
    "print(x_conv1_all.shape)  # Shape should be (number of molecules, channels, sequence_length)\n",
    "\n",
    "# Initialize a list to store the embeddings to be saved later\n",
    "save_emb = []\n",
    "\n",
    "# Define the target vocabulary element to search for (example: 19 corresponds to \"O\")\n",
    "target_vocab = 19\n",
    "\n",
    "# Loop through each molecule and each character in the SMILES strings\n",
    "for each_molecule in range(1792):  # Total number of molecules (combined train + validation)\n",
    "    for each_character in range(X_inp_all.shape[1]):  # Loop over each character in the SMILES string\n",
    "\n",
    "        # Check if the current character matches the target vocabulary element (e.g., \"O\")\n",
    "        if X_inp_all[each_molecule][each_character] == target_vocab:\n",
    "            \n",
    "            # Get the output of the first convolutional layer for the specific molecule and character\n",
    "            final = x_conv1_all[each_molecule, :, each_character]\n",
    "            print(final.shape)  # Print the shape of the extracted convolutional embedding\n",
    "            \n",
    "            # Append the molecule index to the embedding for identification purposes\n",
    "            finalindexed = np.concatenate((final, [each_molecule]))\n",
    "\n",
    "            # Save the indexed embedding for later use\n",
    "            save_emb.append(finalindexed)\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array and save it to a CSV file\n",
    "np.savetxt('conv1embs.csv', save_emb, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
