{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows you to extract embeddings from a trained schnet model, for all layers (interactions residues and built embedding signals)\n",
    "\n",
    "Requirements:\n",
    "obabel 3.1.1\n",
    "schnetpack 0.3\n",
    "\n",
    "\n",
    "NOTE that the model our trained on schnetpack 0.3, therefore it is required to load the model using that version of schnetpack, otherwise it might error out. In schnetpack 0.3, all tensors have to be detached to numpy FIRST, using detach().numpy() whereas in newer versions this is not necessary.\n",
    "\n",
    "If you have your own model that was not trained on 0.3, it can be used here, only you have to delete the detach().numpy() steps as that is no longer required for tensors in later versions of schnet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First, load trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "from schnetpack import AtomsData\n",
    "import numpy as np\n",
    "\n",
    "#Set the same hyperparameters that the schnet model was trained on \n",
    "#Our model's parameters are set below\n",
    "n_atom_basis = 128\n",
    "n_filters = 128\n",
    "n_gaussians = 50\n",
    "n_interactions = 6 \n",
    "cutoff = 50. \n",
    "\n",
    "#Load qm9 data\n",
    "qm9_filepath = 'data/datasets/QM9/qm9.db'\n",
    "qm9_data = QM9(qm9_filepath,download=False,remove_uncharacterized=True)\n",
    "\n",
    "\n",
    "# Load atom ref data \n",
    "atomrefs = qm9_data.get_atomref(QM9.U0)\n",
    "\n",
    "# Define SchNet representation model\n",
    "schnet = spk.representation.SchNet(\n",
    "n_atom_basis=n_atom_basis, n_filters=n_filters, n_gaussians=n_gaussians, n_interactions=n_interactions,\n",
    "cutoff=cutoff , cutoff_network=spk.nn.cutoff.CosineCutoff\n",
    ")\n",
    "\n",
    "# Define SchNet output model and property to be predicted\n",
    "output_U0 = spk.atomistic.Atomwise(n_in=n_filters,atomref=atomrefs[QM9.U0])\n",
    "\n",
    "# Define atomistic model\n",
    "model = spk.AtomisticModel(representation=schnet,output_modules=output_U0)\n",
    "\n",
    "\n",
    "# Load saved checkpoint file\n",
    "checkpoint_path = 'data/trainedmodels/model1/trainingcheckpoints/trained-1000.pth'\n",
    "load_checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "#qm9_i6_30f_20g-1000-500-4_300.pth\n",
    "# load model's state dictionary from saved checkpoint\n",
    "model.load_state_dict(load_checkpoint)\n",
    "\n",
    "\n",
    "#set up device for forward pass\n",
    "device='cpu'\n",
    "\n",
    "# load atoms converter \n",
    "converter = spk.data.AtomsConverter(device=device)\n",
    "\n",
    "#This will show you all the available layers\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hook function for model layer extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This hook function allows you to grab the output of any layer in a schnet model\n",
    "#Using the \"register_forward_hook\" function on the layers of the loaded model \n",
    "def hook(self, inp_tensor, out_tensor):\n",
    "    # Self is included and refers to the model class\n",
    "    # Global allows us to utilize embedding_output outside the current function scope\n",
    "    global layer\n",
    "    #Update the embedding_output variable to be equal to our output tensor\n",
    "    layer=out_tensor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract embeddings from each layer in schnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from label.manuallabel2 import labeller\n",
    "from label.manuallabel2.utils import utils\n",
    "\n",
    "#Define number of molecule you want to extract (or range)\n",
    "molecule_range = [0,100]\n",
    "\n",
    "# Define column headings, 128 will be for embedding features extracted, other columns will be for other important information of the atom\n",
    "#Which molecule is atom from (molecule_index), the index of the atom (atom_index), the xyz coordinats of atom, and\n",
    "#the functional group label of the atom, with a gnuplot marker and a decimal color\n",
    "column_headings = ['emb%s' %(each_feature) for each_feature in range(n_atom_basis)]\n",
    "column_headings.append('molecule_index')\n",
    "column_headings.append('element')\n",
    "column_headings.append('x_coord')\n",
    "column_headings.append('y_coord')\n",
    "column_headings.append('z_coord')\n",
    "column_headings.append('fg_label')\n",
    "column_headings.append('fg_gnuplotmarker')\n",
    "column_headings.append('fg_decimalcolor')\n",
    "column_headings.append('layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize an empty list that will hold all the atom's embedding & other relevant info about the atom\n",
    "#This list will be stacked using pd.concat\n",
    "atom_emb_dataframes = []\n",
    "atom_int_dataframes = []\n",
    "\n",
    "#Run through each molecule defined in the range\n",
    "for molecule_index in range(molecule_range[0],molecule_range[1]):\n",
    "\n",
    "    #simple load bar, shows progress every 100 molecules passed\n",
    "    if molecule_index % 100 == 0:\n",
    "        print('molecule_index',molecule_index)\n",
    "\n",
    "\n",
    "    #Load molecule's properties\n",
    "    at, props = qm9_data.get_properties(molecule_index)\n",
    "    xyz_positions = props['_positions'].detach().numpy()\n",
    "    atomic_numbers = props['_atomic_numbers'].detach().numpy()\n",
    "    number_atoms = len(atomic_numbers)\n",
    "\n",
    "    #Convert to schnet-ready input\n",
    "    inputs = converter(at)\n",
    "\n",
    "    #write xyz and mol file in a temp file (important for labelling the functional groups)\n",
    "    mol_filename = utils.xyz2mol(props)\n",
    "\n",
    "\n",
    "    #Extract embedding and interaction data per molecule\n",
    "    #run the model in inputs and obtain the \n",
    "    embs_all_layers = []\n",
    "    ints_all_layers = []\n",
    "    for layer_index in range(n_interactions):\n",
    "\n",
    "        #set initial emb \n",
    "        if layer_index == 0:\n",
    "            layer = None \n",
    "            model.representation.embedding.register_forward_hook(hook)\n",
    "            model(inputs)\n",
    "            emb = layer.clone()\n",
    "            emb = layer.detach().numpy()\n",
    "            \n",
    "            embs_all_layers.append(emb)\n",
    "\n",
    "\n",
    "        if layer_index > 0:\n",
    "            layer = None\n",
    "            model.representation.interactions[layer_index-1].register_forward_hook(hook)\n",
    "            model(inputs)\n",
    "            int = layer.clone()\n",
    "            int = layer.detach().numpy()\n",
    "\n",
    "            emb = emb + int\n",
    "\n",
    "            embs_all_layers.append(emb)\n",
    "            ints_all_layers.append(int)\n",
    "\n",
    "\n",
    "    #Run the molecule through each layer, get the output of the layer,\n",
    "    #FOR EACH ATOM, label the atom's functional group, the layer,... put all this information in the output file\n",
    "    for atom_index in range(number_atoms):\n",
    "        #run labeller code to output LDA label, gnuplot color label, gnuplot marker label... \n",
    "        fg_key,fg_label,decimal_color,gnu_marker,hex_color= labeller.label(mol_filename,number_atoms,atom_index,atomic_numbers[atom_index],molecule_index)\n",
    "\n",
    "        for layer_index in range(n_interactions):\n",
    "            #Append other relevant information about the atom on top of the extracted embedding\n",
    "            #As a row which will be stacked to the embs dataframe and saved\n",
    "            embs_atom = np.append(embs_all_layers[layer_index][0][atom_index][0:128],molecule_index)\n",
    "            embs_atom = np.append(embs_atom,atomic_numbers[atom_index])\n",
    "            embs_atom = np.append(embs_atom,xyz_positions[atom_index][0])\n",
    "            embs_atom = np.append(embs_atom,xyz_positions[atom_index][1])\n",
    "            embs_atom = np.append(embs_atom,xyz_positions[atom_index][2])\n",
    "            embs_atom = np.append(embs_atom,fg_label)\n",
    "            embs_atom = np.append(embs_atom,gnu_marker)\n",
    "            embs_atom = np.append(embs_atom,decimal_color)\n",
    "            embs_atom = np.append(embs_atom,layer_index) \n",
    "\n",
    "            #Create a new DataFrame from the NumPy array\n",
    "            embs_atom_df = pd.DataFrame(embs_atom.reshape(1, -1),columns = column_headings)\n",
    "            embs_atom_df.loc[0,'fg_key'] = fg_key\n",
    "            embs_atom_df.loc[0,'fg_hexcolor'] = hex_color\n",
    "\n",
    "            #Append the DataFrame to the list\n",
    "            atom_emb_dataframes.append(embs_atom_df)\n",
    "    \n",
    "        #interactions only come after embedding layer 0, so there are only 5, or n_interactions - 1 of them\n",
    "        if layer_index > 0:\n",
    "            for atom_index in range(number_atoms):\n",
    "\n",
    "                #Append other relevant information about the atom on top of the extracted embedding\n",
    "                #As a row which will be stacked to the embs dataframe and saved\n",
    "                ints_atom = np.append(ints_all_layers[layer_index-1][0][atom_index][0:128],molecule_index)\n",
    "                ints_atom = np.append(ints_atom,atomic_numbers[atom_index])\n",
    "                ints_atom = np.append(ints_atom,xyz_positions[atom_index][0])\n",
    "                ints_atom = np.append(ints_atom,xyz_positions[atom_index][1])\n",
    "                ints_atom = np.append(ints_atom,xyz_positions[atom_index][2])\n",
    "                ints_atom = np.append(ints_atom,fg_label)\n",
    "                ints_atom = np.append(ints_atom,gnu_marker)\n",
    "                ints_atom = np.append(ints_atom,decimal_color)\n",
    "                ints_atom = np.append(ints_atom,layer_index-1) \n",
    "\n",
    "                #Create a new DataFrame from the NumPy array\n",
    "                ints_atom_df = pd.DataFrame(ints_atom.reshape(1, -1), columns = column_headings)\n",
    "                ints_atom_df.loc[0,'fg_key'] = fg_key\n",
    "                ints_atom_df.loc[0,'fg_hexcolor'] = hex_color\n",
    "\n",
    "\n",
    "                #Append the DataFrame to the list\n",
    "                atom_int_dataframes.append(ints_atom_df)    \n",
    "        \n",
    "embs_df = pd.concat(atom_emb_dataframes, ignore_index=True)\n",
    "ints_df = pd.concat(atom_int_dataframes, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings/interactions of every layer (and every atom, labelled) to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df.to_csv('data/embs/model1/qm9_first1000/embs_all_layers&atoms.csv',index=False)\n",
    "ints_df.to_csv('data/embs/model1/qm9_first1000/ints_all_layers&atoms.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate embeddings of an atom-type (element) and a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the layer and element to analyze with dimension reduction and lda \n",
    "layer = 5   \n",
    "element = 1\n",
    "\n",
    "# Filter rows based on the value in the 'Category' column\n",
    "filtered_df = embs_df[(embs_df['layer'] == layer) & (embs_df['element'] == element)]\n",
    "\n",
    "# Save the filtered rows to a new CSV file\n",
    "filtered_df.to_csv('data/embs/model1/qm9_first1000/layer5_elementO/embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality-reduction using PCA and t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA on embedding/interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "filtered_df = pd.read_csv('data/embs/model1/qm9_first1000/layer5_elementO/embs.csv')\n",
    "\n",
    "#PCA parameters\n",
    "n_features = 128\n",
    "n_components = 128\n",
    "scale_data = False\n",
    "\n",
    "X = filtered_df.iloc[:,0:n_features]\n",
    "print(X)\n",
    "\n",
    "if scale_data == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X,random_state=100)\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(random_state=100,n_components=n_components)\n",
    "pca.fit(X)\n",
    "\n",
    "x_pca = pca.transform(X.iloc[:,0:n_features])\n",
    "\n",
    "#get the eigenvalues and eigenvectors of covariance matrix for analysis\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the PCA dimension reduction with hexadecimal colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot PCA dimension reduction \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(filtered_df.iloc[:,138])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1],color=filtered_df.iloc[:,138].tolist(), marker='o')\n",
    "\n",
    "# Optionally, you can add labels and a title\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "\n",
    "# Show a legend if needed\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "perp = 50\n",
    "X = filtered_df.iloc[:,0:n_features]\n",
    "\n",
    "X_tsne = TSNE(n_components=2,perplexity=perp).fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot t-SNE dimension reduction \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(filtered_df.iloc[:,138])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1],color=filtered_df.iloc[:,138].tolist(), marker='o')\n",
    "\n",
    "# Optionally, you can add labels and a title\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "\n",
    "# Show a legend if needed\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filtered_df = pd.read_csv('data/embs/model1/qm9_first1000/layer5_elementO/embs.csv')\n",
    "n_features = 128\n",
    "\n",
    "#Y is the fg label in column 133\n",
    "X = filtered_df.iloc[:,0:n_features]\n",
    "y = filtered_df.iloc[:,133].values\n",
    "\n",
    "#First clean up the data, any class that has one data point cannot be split, has to be remove\n",
    "#All labels must be shifted to go from 0--->N-labels\n",
    "# Count the number of data points per clas\n",
    "class_counts = np.bincount(y.astype(int))\n",
    "\n",
    "# Identify classes with only one data point\n",
    "classes_to_remove = np.where(class_counts == 1)[0]\n",
    "\n",
    "# Remove data points from classes with only one data point\n",
    "for class_to_remove in classes_to_remove:\n",
    "    mask = y != class_to_remove\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "# Renumber the classes to make them consecutive\n",
    "unique_classes = np.unique(y)\n",
    "class_mapping = {old_class: new_class for new_class, old_class in enumerate(unique_classes)}\n",
    "y = np.vectorize(class_mapping.get)(y)\n",
    "\n",
    "#split data, while ensuring each split gets all classes (Stratify)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=30)    \n",
    "        \n",
    "LDA = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "X_fit = LDA.fit(X_train,y_train)\n",
    "\n",
    "y_pred = X_fit.predict(X_test)\n",
    "\n",
    "\n",
    "print('LDA accuracy: ', LDA.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d8435ccd79177254b883c2dec0668c8ed2f1e4b0134a8204790627afd7a6d86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
