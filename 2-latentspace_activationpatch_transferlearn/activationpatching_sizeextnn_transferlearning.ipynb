{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size-extensive neural net\n",
    "\n",
    "this neural net takes in molecules-embs of different sizes makes a proposition about each atomistic contribution to a size-extensive property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amerelsamman/anaconda3/envs/dlchem/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(44)\n",
    "\n",
    "'''\n",
    "Last Modified: 2024/10/02\n",
    "\n",
    "Simple architecture of a size-extensive neural network which uses \n",
    "latent space of a pretrained model to fine-tune and make predictions\n",
    "\n",
    "    atomwise_nn\n",
    "        - the class that will define the neural network \n",
    "        architecture for size-extensive activation \n",
    "        patching transfer learning \n",
    "'''\n",
    "\n",
    "\n",
    "#ATOMWISE size extensive neural network class\n",
    "class atomwise_nn(nn.Module):\n",
    "    '''\n",
    "    Defininig size-extensive activation patching \n",
    "    transfer learning model architecture to train \n",
    "\n",
    "        INPUT_SIZE\n",
    "            - the number dimensions in the input feature space\n",
    "        HIDDEN_SIZE\n",
    "            - the number of parameters in the one hidden layer model\n",
    "        OUTPUT_SIZE\n",
    "            - the number of dimensions in the output feature space \n",
    "              (1 for scalar)\n",
    "\n",
    "    Returns:\n",
    "        sizeext_quantity\n",
    "            - the final output quantity as predicted by the network\n",
    "    '''\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE):\n",
    "        super(atomwise_nn, self).__init__()\n",
    "        self.INPUT_SIZE = INPUT_SIZE\n",
    "        self.HIDDEN_SIZE = HIDDEN_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(INPUT_SIZE, HIDDEN_SIZE).double()\n",
    "        self.fc2 = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        sizeext_quantity = 0\n",
    "        for each_atomemb in range(len(x)):\n",
    "            # Forward pass through the network\n",
    "            emb = x[each_atomemb]\n",
    "            emb = F.relu(self.fc1(emb))\n",
    "            emb = self.fc2(emb)\n",
    "\n",
    "            #sum the atomwise outputs for each latent space vector (embedding)\n",
    "            #going through the atomwise neural network\n",
    "            sizeext_quantity = sizeext_quantity + emb\n",
    "\n",
    "        return sizeext_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training atomwise_nn\n",
    "\n",
    "This is where you define the exact size of the architecture, \n",
    "and other hyperparameters associated \n",
    "\n",
    "    INPUT_SIZE\n",
    "        - number of dimensions in input latent space (embeddings)\n",
    "    HIDDEN_SIZE\n",
    "        - number of parameteres in the one-layer size-extensive model\n",
    "    OUTPUT_SIZE\n",
    "        - number of output dimensions of the final molecular property\n",
    "    LEARNING_RATE\n",
    "        - the rate of learning\n",
    "    NUM_EPOCHS\n",
    "        - number of epochs before the training loop finished  \n",
    "    NUM_TRAIN_SAMPLES\n",
    "        - number of training molecules\n",
    "    NUM_VAL_SAMPLES\n",
    "        - number of validation molecules\n",
    "    EMBS_PATH\n",
    "        - where the latent vectors (embeddings) filepath is located\n",
    "    MOL_PROPERTY_PATH\n",
    "        - where the molecular property filpath is located\n",
    "'''\n",
    "\n",
    "# Define hyperparameters\n",
    "INPUT_SIZE = 128\n",
    "HIDDEN_SIZE = 200\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10000\n",
    "NUM_TRAIN_SAMPLES = 450\n",
    "NUM_VAL_SAMPLES = 100\n",
    "\n",
    "EMBS_PATH = '../data/datasets/embsMP/embslayer5.csv'\n",
    "N_EMBS_FEATURES = 128\n",
    "MOL_PROPERTY_PATH = '../data/datasets/embsMP/mps.csv'\n",
    "\n",
    "model = atomwise_nn(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Define the loss function (criterion) and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#read in the latent space data and the final property data to train on \n",
    "embs = pd.read_csv(EMBS_PATH)\n",
    "mps_true = pd.read_csv(MOL_PROPERTY_PATH)\n",
    "\n",
    "embs_features = embs.iloc[:,0:N_EMBS_FEATURES].values\n",
    "normalize_embs_128 = nn.BatchNorm1d(N_EMBS_FEATURES).double()\n",
    "embs_featuresnorm = normalize_embs_128(torch.tensor(embs_features))\n",
    "\n",
    "embs_norm = np.hstack((embs_featuresnorm.detach().numpy(),embs.iloc[:,N_EMBS_FEATURES:].values))\n",
    "\n",
    "print(np.shape(embs_norm))\n",
    "\n",
    "'''\n",
    "Run Training Loop\n",
    "\n",
    "'''\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_train_loss = 0.0\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for each_molecule in range(NUM_TRAIN_SAMPLES):\n",
    "        # Generate random data for each batch\n",
    "        X = embs_norm[embs_norm[:,128] == each_molecule]\n",
    "        X = X[:,0:128]\n",
    "        y = mps_true.iloc[each_molecule].values\n",
    "        \n",
    "        # Convert data to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X)\n",
    "        y_tensor = torch.tensor(y)\n",
    "\n",
    "        # Zero gradients, forward pass, backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_tensor)\n",
    "\n",
    "        loss = criterion(output, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for this epoch\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    truths = []\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for each_molecule in range(NUM_VAL_SAMPLES):\n",
    "            # Generate random data for each batch\n",
    "            X = embs_norm[embs_norm[:,128] == each_molecule]\n",
    "            X = X[:,0:128]\n",
    "            y = mps_true.iloc[each_molecule].values\n",
    "            \n",
    "            # Convert data to PyTorch tensors\n",
    "            X_tensor = torch.tensor(X)\n",
    "            y_tensor = torch.tensor(y)\n",
    "\n",
    "            # Zero gradients, forward pass, backward pass, and update weights\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_tensor)\n",
    "\n",
    "            loss = criterion(output, y_tensor)\n",
    "\n",
    "            # Accumulate loss for this epoch\n",
    "            total_val_loss += loss.item()            \n",
    "\n",
    "            outputs.append(output)\n",
    "            truths.append(y)\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    average_train_loss = total_train_loss / NUM_TRAIN_SAMPLES\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    average_val_loss = total_val_loss / NUM_VAL_SAMPLES\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Val Loss: {average_val_loss:.4f}\")\n",
    "    \n",
    "    train_loss.append([epoch,average_train_loss])\n",
    "    val_loss.append([epoch,average_val_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
