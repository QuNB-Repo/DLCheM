{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size-extensive neural net\n",
    "\n",
    "this neural net takes in molecules-embs of different sizes makes a proposition about each atomistic contribution to a size-extensive property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amerelsamman/anaconda3/envs/dlchem/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script defines a simple architecture for a size-extensive neural network \n",
    "that uses latent space from a pre-trained model to fine-tune and make predictions. \n",
    "The model is designed to predict a scalar quantity by summing up contributions \n",
    "from individual atom embeddings (atomwise approach). The architecture is useful \n",
    "for transfer learning tasks, where latent space embeddings are derived from a \n",
    "pre-trained model, and predictions are made in a size-extensive manner.\n",
    "\n",
    "Key components:\n",
    "1. **atomwise_nn** class: Defines the architecture of a size-extensive neural \n",
    "   network with two fully connected layers. Each atom embedding is processed \n",
    "   individually, and the outputs are summed to form the final prediction.\n",
    "2. **Forward pass**: Processes each atom's embedding through the network and \n",
    "   sums the outputs to generate a final prediction for the molecular property \n",
    "   (size-extensive quantity).\n",
    "\n",
    "The architecture is well-suited for tasks that involve fine-tuning latent space \n",
    "embeddings derived from pre-trained models for specific predictions (e.g., molecular \n",
    "properties).\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(44)  # Setting a random seed for reproducibility\n",
    "\n",
    "# ATOMWISE size extensive neural network class\n",
    "class atomwise_nn(nn.Module):\n",
    "    '''\n",
    "    Defining size-extensive activation patching \n",
    "    transfer learning model architecture to train.\n",
    "\n",
    "        INPUT_SIZE\n",
    "            - The number of dimensions in the input feature space\n",
    "        HIDDEN_SIZE\n",
    "            - The number of parameters in the hidden layer\n",
    "        OUTPUT_SIZE\n",
    "            - The number of dimensions in the output feature space \n",
    "              (1 for scalar output, like a property prediction)\n",
    "\n",
    "    Returns:\n",
    "        sizeext_quantity\n",
    "            - The final predicted quantity, representing the \n",
    "              size-extensive molecular property.\n",
    "    '''\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE):\n",
    "        super(atomwise_nn, self).__init__()\n",
    "        self.INPUT_SIZE = INPUT_SIZE\n",
    "        self.HIDDEN_SIZE = HIDDEN_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "\n",
    "        # Define two fully connected layers\n",
    "        self.fc1 = nn.Linear(INPUT_SIZE, HIDDEN_SIZE).double()  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE).double()  # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        sizeext_quantity = 0  # Initialize the size-extensive quantity to 0\n",
    "        \n",
    "        # Loop through each atom embedding in the input 'x'\n",
    "        for each_atomemb in range(len(x)):\n",
    "            # Forward pass through the network for each atom embedding\n",
    "            emb = x[each_atomemb]\n",
    "            emb = F.relu(self.fc1(emb))  # Pass through the first layer and apply ReLU activation\n",
    "            emb = self.fc2(emb)          # Pass through the second layer (output layer)\n",
    "\n",
    "            # Sum the outputs of the individual atom embeddings\n",
    "            sizeext_quantity += emb\n",
    "\n",
    "        return sizeext_quantity  # Return the final predicted quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script implements the training process for a size-extensive neural network, \n",
    "specifically the `atomwise_nn` class defined earlier. It uses latent space embeddings \n",
    "from a pre-trained model and fine-tunes them to predict a scalar molecular property \n",
    "(e.g., energy or another physical property). The training process reads embeddings, \n",
    "normalizes them, and trains the model using these embeddings as inputs and molecular \n",
    "properties as targets.\n",
    "\n",
    "Key components:\n",
    "1. **Hyperparameter definitions**: Defines key parameters like input size, hidden size, \n",
    "   learning rate, and the number of epochs.\n",
    "2. **Loading data**: Reads latent space embeddings and target molecular properties from \n",
    "   CSV files.\n",
    "3. **Training loop**: Iteratively trains the model using backpropagation, computing the \n",
    "   loss for both training and validation data, and updating the model weights.\n",
    "4. **Validation**: After each epoch, the validation loss is calculated to assess the \n",
    "   model's performance on unseen data.\n",
    "\n",
    "The script is designed to train on embeddings and molecular property data, and the \n",
    "trained model can later be used to predict scalar properties for new inputs.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(44)\n",
    "\n",
    "'''\n",
    "Training atomwise_nn\n",
    "\n",
    "This section defines the architecture and hyperparameters for the \n",
    "size-extensive neural network (`atomwise_nn`) and loads the embeddings \n",
    "(latent space) and molecular properties from CSV files.\n",
    "\n",
    "    INPUT_SIZE\n",
    "        - Number of dimensions in input latent space (embeddings)\n",
    "    HIDDEN_SIZE\n",
    "        - Number of parameters in the hidden layer of the model\n",
    "    OUTPUT_SIZE\n",
    "        - Number of output dimensions of the final molecular property\n",
    "    LEARNING_RATE\n",
    "        - Learning rate for the optimizer\n",
    "    NUM_EPOCHS\n",
    "        - Number of epochs to run during training\n",
    "    NUM_TRAIN_SAMPLES\n",
    "        - Number of training molecules\n",
    "    NUM_VAL_SAMPLES\n",
    "        - Number of validation molecules\n",
    "    EMBS_PATH\n",
    "        - Filepath where latent vectors (embeddings) are stored\n",
    "    MOL_PROPERTY_PATH\n",
    "        - Filepath where molecular property values are stored\n",
    "'''\n",
    "\n",
    "# Define hyperparameters\n",
    "INPUT_SIZE = 128\n",
    "HIDDEN_SIZE = 200\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10000\n",
    "NUM_TRAIN_SAMPLES = 450\n",
    "NUM_VAL_SAMPLES = 100\n",
    "\n",
    "EMBS_PATH = '../data/datasets/embsMP/embslayer5.csv'  # Embeddings file\n",
    "N_EMBS_FEATURES = 128  # Number of embedding features\n",
    "MOL_PROPERTY_PATH = '../data/datasets/embsMP/mps.csv'  # Molecular property file\n",
    "\n",
    "# Initialize the model\n",
    "model = atomwise_nn(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Define the loss function (Mean Squared Error) and the optimizer (Adam)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Load the latent space embeddings and molecular property data\n",
    "embs = pd.read_csv(EMBS_PATH)\n",
    "mps_true = pd.read_csv(MOL_PROPERTY_PATH)\n",
    "\n",
    "# Normalize the embeddings\n",
    "embs_features = embs.iloc[:, 0:N_EMBS_FEATURES].values\n",
    "normalize_embs_128 = nn.BatchNorm1d(N_EMBS_FEATURES).double()\n",
    "embs_featuresnorm = normalize_embs_128(torch.tensor(embs_features))\n",
    "\n",
    "# Combine normalized embeddings with the rest of the data\n",
    "embs_norm = np.hstack((embs_featuresnorm.detach().numpy(), embs.iloc[:, N_EMBS_FEATURES:].values))\n",
    "\n",
    "print(np.shape(embs_norm))  # Check the shape of the combined embedding data\n",
    "\n",
    "'''\n",
    "Run Training Loop\n",
    "\n",
    "The training loop runs for a defined number of epochs (`NUM_EPOCHS`). \n",
    "In each epoch, it performs the following steps:\n",
    "1. **Training**: \n",
    "   - For each molecule, it computes the loss and updates model weights using backpropagation.\n",
    "2. **Validation**: \n",
    "   - Evaluates the model on a validation set at the end of each epoch.\n",
    "\n",
    "For each epoch, the average training and validation losses are printed.\n",
    "'''\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_train_loss = 0.0  # Accumulate training loss for the epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = []  # Store training loss values\n",
    "    val_loss = []  # Store validation loss values\n",
    "    \n",
    "    # Training phase\n",
    "    for each_molecule in range(NUM_TRAIN_SAMPLES):\n",
    "        # Select the latent space embedding for the current molecule\n",
    "        X = embs_norm[embs_norm[:, 128] == each_molecule]\n",
    "        X = X[:, 0:128]\n",
    "        y = mps_true.iloc[each_molecule].values  # Target molecular property\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X)\n",
    "        y_tensor = torch.tensor(y)\n",
    "\n",
    "        # Zero gradients, forward pass, compute loss, backward pass, and update weights\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "        output = model(X_tensor)  # Forward pass\n",
    "        loss = criterion(output, y_tensor)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        total_train_loss += loss.item()  # Accumulate loss for this epoch\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    outputs = []\n",
    "    truths = []\n",
    "    total_val_loss = 0.0  # Accumulate validation loss for the epoch\n",
    "\n",
    "    with torch.no_grad():  # No gradient computation in validation\n",
    "        for each_molecule in range(NUM_VAL_SAMPLES):\n",
    "            # Select the latent space embedding for the validation molecule\n",
    "            X = embs_norm[embs_norm[:, 128] == each_molecule]\n",
    "            X = X[:, 0:128]\n",
    "            y = mps_true.iloc[each_molecule].values  # Target molecular property\n",
    "            \n",
    "            # Convert data to PyTorch tensors\n",
    "            X_tensor = torch.tensor(X)\n",
    "            y_tensor = torch.tensor(y)\n",
    "\n",
    "            # Forward pass and compute validation loss\n",
    "            output = model(X_tensor)\n",
    "            loss = criterion(output, y_tensor)\n",
    "            total_val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "            outputs.append(output)\n",
    "            truths.append(y)\n",
    "\n",
    "    # Print the average loss for the training and validation phases\n",
    "    average_train_loss = total_train_loss / NUM_TRAIN_SAMPLES\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "    average_val_loss = total_val_loss / NUM_VAL_SAMPLES\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Val Loss: {average_val_loss:.4f}\")\n",
    "    \n",
    "    # Append the training and validation losses to the respective lists\n",
    "    train_loss.append([epoch, average_train_loss])\n",
    "    val_loss.append([epoch, average_val_loss])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
