{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load data file(s)\n",
    "data_filepath = '../data/embs/modelbreak/10-2-128-50-6-50/Oembslayer5-110000-120000.csv'\n",
    "data = pd.read_csv(data_filepath,delimiter=',')\n",
    "n_features = 128\n",
    "\n",
    "#Define save filedir\n",
    "save_filedir ='../data/embs/modelbreak/10-2-128-50-6-50/'\n",
    "\n",
    "#2325\n",
    "#3255\n",
    "\n",
    "# Filter out classes with only one sample\n",
    "class_counts = data['ldalabel'].value_counts()\n",
    "single_sample_classes = class_counts[class_counts == 1].index.tolist()\n",
    "\n",
    "data_filtered = data[~data['ldalabel'].isin(single_sample_classes)]\n",
    "\n",
    "#Define X and y for the classification task\n",
    "X = data_filtered.iloc[:,0:n_features].values\n",
    "y = data_filtered['ldalabel'].values\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9206810e+00 -3.2983106e-01  7.5627910e-01 ...  1.3296798e-01\n",
      "  -6.8067586e-01 -1.2198466e+00]\n",
      " [ 2.0319133e+00 -4.3267850e-01  7.8603300e-01 ...  1.8094125e-01\n",
      "  -6.6059010e-01 -1.3792698e+00]\n",
      " [ 1.8612910e+00 -6.4994854e-01  5.4668593e-01 ... -1.2576580e-03\n",
      "  -5.9963983e-01 -1.4581711e+00]\n",
      " ...\n",
      " [ 4.1770820e+00 -4.8849255e-01  2.0907507e+00 ...  1.6687960e+00\n",
      "   2.7272210e+00 -3.1752954e+00]\n",
      " [ 3.7086596e+00 -7.1281374e-01  1.5987878e+00 ...  1.0452396e+00\n",
      "   1.9527593e+00 -3.1408680e+00]\n",
      " [ 2.1647525e+00 -3.4640655e-01  1.3032643e+00 ...  3.7909890e-01\n",
      "   4.7228730e-01 -1.0450515e+00]]\n",
      "[ 5 16  5 ...  1  1  5]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialize linear discriminant analysis state \n",
    "#(this maintains the coefficients, intercept, priors, covariance, accuracy score,... used during fitting)\n",
    "lda = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "#Perfoem lda fit\n",
    "X_ldafit = lda.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97416262]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      8043\n",
      "           3       0.88      0.91      0.89       592\n",
      "           4       0.86      0.92      0.89      1183\n",
      "           5       0.99      0.97      0.98      3753\n",
      "          15       0.96      0.97      0.96      1092\n",
      "          16       0.98      0.94      0.96       799\n",
      "          17       0.96      0.94      0.95       690\n",
      "          18       0.95      0.95      0.95       477\n",
      "          19       0.83      0.91      0.87       199\n",
      "          20       0.89      0.97      0.93        96\n",
      "          21       0.91      0.98      0.95        97\n",
      "          22       0.74      1.00      0.85        17\n",
      "          23       0.99      1.00      0.99        69\n",
      "\n",
      "    accuracy                           0.97     17107\n",
      "   macro avg       0.92      0.96      0.94     17107\n",
      "weighted avg       0.98      0.97      0.97     17107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(np.array([lda.score(X,y)]))\n",
    "#Save important results (accuracy, coeffs... etc.)\n",
    "\n",
    "y_pred = lda.predict(X)\n",
    "# Print classification report which contains precision, recall, f1-score, and support per class\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test LDA with new X points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = -2*np.eye(128)\n",
    "print(data)\n",
    "\n",
    "\n",
    "predictions = lda.predict(data)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "#IDEA WHY DON'T YOU MAKE AN ALGORITHM, THAT CONTINUOUSLY BLEEDS 100 OVER\n",
    "#TO THE REST OF THE VECTOR IN THE COLUMN,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for each_class in range(25):\n",
    "    print(each_class)\n",
    "    w = (np.where(predictions==each_class))\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data file(s)\n",
    "data_filepath = '../data/embs/model1-10000/layer5/Oembs/embs-NOOUTLIERSpca.csv'\n",
    "data = pd.read_csv(data_filepath,delimiter=',')\n",
    "\n",
    "#The question you thus have to ask yourself is where is the vector that takes you to the middle of the category of LDA prediction. \n",
    "#For example vector 0,0,0,0,0 vector seems to be the middle of the 2 category because once you are at around that vector \n",
    "#You will always be category 2\n",
    "\n",
    "#How do we find that? we run all of LDA predictions of each training class, \n",
    "#get the mean vector of the predictions, \n",
    "#and see if the mean vector also gives you the same class\n",
    "#And everything around the mean vector gives the same class\n",
    "\n",
    "mean_classes = []\n",
    "predictions_of_classes = [] \n",
    "for each_class in range(25):\n",
    "\n",
    "    if each_class != 0 and each_class != 6 and each_class != 14:\n",
    "\n",
    "        X_train_class = data[data.iloc[:,133] == each_class].values\n",
    "\n",
    "        mean_emb_class = np.mean(X_train_class[:,:3],axis=0)\n",
    "\n",
    "        predictions = lda.predict(mean_emb_class.reshape(1,-1))\n",
    "\n",
    "        mean_classes.append(mean_emb_class)\n",
    "        print(predictions)\n",
    "\n",
    "print(mean_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for each mean class, figure out how much uncertainty is in that class,\n",
    "mean_class_fg2 = mean_classes[0]\n",
    "\n",
    "true_prediction = lda.predict(mean_class_fg2.reshape(1,-1))\n",
    "#print(variant_vector)\n",
    "print(true_prediction)\n",
    "\n",
    "target_features = 3\n",
    "\n",
    "#Find the variance of each embedding parameter that keeps within the same prediction\n",
    "variance_step = 0.1\n",
    "n_steps = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing a variance in each feature until the category changes. Finding out the variance for each feature around each class of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#There are two ways we can do this (that I can think of)\n",
    "#1) not-safe extreme - let each feature vary on its own, independent of the rest, until the prediction breaks\n",
    "#2) safe extreme - vary ALL features at once, until a prediction breaks... take that as the minimum range for the class\n",
    "\n",
    "\n",
    "#1) case\n",
    "#print(mean_class_fg2)\n",
    "mean_class_fg2_copy = mean_class_fg2.copy()\n",
    "\n",
    "for each_target_feature in range(target_features):\n",
    "    variant_vector = np.zeros((3))\n",
    "    total_variance = 0\n",
    "\n",
    "    for steps in range(n_steps):\n",
    "        total_variance = total_variance + variance_step \n",
    "        for each_feature in range(3):\n",
    "            if each_feature == each_target_feature:\n",
    "                variant_vector[each_feature] = mean_class_fg2_copy[each_feature] + total_variance\n",
    "            else:\n",
    "                variant_vector[each_feature] = mean_class_fg2_copy[each_feature]\n",
    "\n",
    "        prediction = lda.predict(variant_vector.reshape(1,-1))\n",
    "\n",
    "        #    mean_classes_fg_2 = mean_classes_fg_2 + np.zeros((1,128)) \n",
    "        if prediction != true_prediction:\n",
    "#            print(true_prediction)\n",
    "#            print(prediction)\n",
    "            break\n",
    "    print(each_target_feature)\n",
    "    print(total_variance)\n",
    "\n",
    "    #see if these correspond with the values above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) case\n",
    "\n",
    "#There are two ways we can do this (that I can think of)\n",
    "#1) not-safe extreme - let each feature vary on its own, independent of the rest, until the prediction breaks\n",
    "#2) safe extreme - vary ALL features at once, until a prediction breaks... take that as the minimum range for the class\n",
    "\n",
    "\n",
    "#Now for each mean class, figure out how much uncertainty is in that class,\n",
    "mean_class_fg2 = mean_classes[4]\n",
    "\n",
    "true_prediction = lda.predict(mean_class_fg2.reshape(1,-1))\n",
    "#print(variant_vector)\n",
    "print(true_prediction)\n",
    "\n",
    "target_features = 128\n",
    "\n",
    "#Find the variance of each embedding parameter that keeps within the same prediction\n",
    "variance_step = 0.1\n",
    "n_steps = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#2) case\n",
    "#print(mean_class_fg2)\n",
    "mean_class_fg2_copy = mean_class_fg2.copy()\n",
    "\n",
    "variant_vector = np.zeros((128))\n",
    "total_variance = 0\n",
    "\n",
    "for steps in range(n_steps):\n",
    "    total_variance = total_variance + variance_step \n",
    "    for each_feature in range(128):\n",
    "        variant_vector[each_feature] = mean_class_fg2_copy[each_feature] - total_variance\n",
    "\n",
    "    prediction = lda.predict(variant_vector.reshape(1,-1))\n",
    "\n",
    "    #    mean_classes_fg_2 = mean_classes_fg_2 + np.zeros((1,128)) \n",
    "    if prediction != true_prediction:\n",
    "        print(true_prediction)\n",
    "        print(prediction)\n",
    "        break\n",
    "\n",
    "print(total_variance)\n",
    "\n",
    "    #see if these correspond with the values above\n",
    "\n",
    "lower_bound = mean_class_fg2 - total_variance + 0.1\n",
    "lda.predict(lower_bound.reshape(1,-1))\n",
    "\n",
    "print(mean_class_fg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "for each_data in range(len(data)):\n",
    "\n",
    "    label_key = data.iloc[each_data,133]\n",
    "    colormarker_values = (data.iloc[each_data,134], data.iloc[each_data,135])\n",
    "\n",
    "    labels[label_key] = colormarker_values   \n",
    "\n",
    "print(labels)\n",
    "\n",
    "# Convert the dictionary to a list of lists\n",
    "list_of_labels = [[key, *values] for key, values in labels.items()]\n",
    "\n",
    "# Now, list_of_lists contains a list of lists where each sub-list has the key and its associated values\n",
    "print(list_of_labels)\n",
    "\n",
    "coefs_labeled = np.hstack((lda.coef_,list_of_labels))\n",
    "\n",
    "\n",
    "coef_savefilepath = save_filedir + 'LDAcoefslabeled.csv'\n",
    "np.savetxt(coef_savefilepath,coefs_labeled,delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_savefilepath = save_filedir + 'int.csv'\n",
    "np.savetxt(intercept_savefilepath,lda.intercept_,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
