{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple feedforward neural net for tabular regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load data into a Pandas DataFrame\n",
    "x_data = genfromtxt('../data/embs/embsNMR-brooke/1layerNN-1L1NLbias258/CembsNPMRDNMR.csv',delimiter=',',encoding='utf-8-sig',skip_header=1)\n",
    "\n",
    "# Split data into X and y\n",
    "X = x_data[:,0:128]\n",
    "#128 --> pka & nmr\n",
    "#138 onwards --> electron density \n",
    "y = x_data[:,128]\n",
    "\n",
    "#label to split as well\n",
    "label = x_data[:,129:]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_valtest, y_train, y_valtest, label_train, label_valtest = train_test_split(X, y, label, test_size=0.25, random_state=50)\n",
    "# Split the dataset into training and testing sets\n",
    "X_val, X_test, y_val, y_test, label_val, label_test = train_test_split(X_valtest, y_valtest, label_valtest, test_size=0.5, random_state=42)\n",
    "\n",
    "#input features\n",
    "n_features = 128\n",
    "\n",
    "#Save file directory\n",
    "save_dir = '../data/embs/embspKA-stef/1layerNN-1L1NLbias258/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pytorch model layers, number of nodes in each layer, activation,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (lin2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt, savetxt\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(n_features, 128)\n",
    "        self.lin2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "print(len(params[0])+ len(params[1])+len(params[2])+len(params[3]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters\n",
    "Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0001\n",
    "n_epochs = 10000\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training on Defined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Loss: 6661.51806640625\n",
      "Training R2 -97385.86867471927\n",
      "Validation Loss: 5930.50244140625\n",
      "Epoch 51/10000, Loss: 6464.9560546875\n",
      "Training R2 -32468.373303058983\n",
      "Validation Loss: 5748.31494140625\n",
      "Epoch 101/10000, Loss: 6205.60888671875\n",
      "Training R2 -9456.983909845907\n",
      "Validation Loss: 5505.9326171875\n",
      "Epoch 151/10000, Loss: 5863.6787109375\n",
      "Training R2 -3206.3600134471394\n",
      "Validation Loss: 5188.63232421875\n",
      "Epoch 201/10000, Loss: 5466.802734375\n",
      "Training R2 -1297.1060944030557\n",
      "Validation Loss: 4820.12060546875\n",
      "Epoch 251/10000, Loss: 5035.20654296875\n",
      "Training R2 -591.0142677716427\n",
      "Validation Loss: 4420.71484375\n",
      "Epoch 301/10000, Loss: 4585.31689453125\n",
      "Training R2 -294.3713164802866\n",
      "Validation Loss: 4006.251953125\n",
      "Epoch 351/10000, Loss: 4132.01416015625\n",
      "Training R2 -156.86421444928447\n",
      "Validation Loss: 3591.00341796875\n",
      "Epoch 401/10000, Loss: 3689.296630859375\n",
      "Training R2 -88.04128299462012\n",
      "Validation Loss: 3188.382080078125\n",
      "Epoch 451/10000, Loss: 3267.763916015625\n",
      "Training R2 -51.2976237044687\n",
      "Validation Loss: 2808.352294921875\n",
      "Epoch 501/10000, Loss: 2879.763671875\n",
      "Training R2 -30.78289230599971\n",
      "Validation Loss: 2461.69873046875\n",
      "Epoch 551/10000, Loss: 2533.413330078125\n",
      "Training R2 -18.90279189038206\n",
      "Validation Loss: 2154.616943359375\n",
      "Epoch 601/10000, Loss: 2231.091552734375\n",
      "Training R2 -11.793020328281413\n",
      "Validation Loss: 1887.8297119140625\n",
      "Epoch 651/10000, Loss: 1971.6510009765625\n",
      "Training R2 -7.480745131184557\n",
      "Validation Loss: 1659.3477783203125\n",
      "Epoch 701/10000, Loss: 1751.942626953125\n",
      "Training R2 -4.798092029080122\n",
      "Validation Loss: 1465.331298828125\n",
      "Epoch 751/10000, Loss: 1566.4388427734375\n",
      "Training R2 -3.071076937553233\n",
      "Validation Loss: 1299.8486328125\n",
      "Epoch 801/10000, Loss: 1409.0009765625\n",
      "Training R2 -1.9277411675978247\n",
      "Validation Loss: 1156.4031982421875\n",
      "Epoch 851/10000, Loss: 1275.7431640625\n",
      "Training R2 -1.1667079578225739\n",
      "Validation Loss: 1032.247314453125\n",
      "Epoch 901/10000, Loss: 1162.952392578125\n",
      "Training R2 -0.6474690451436123\n",
      "Validation Loss: 924.0528564453125\n",
      "Epoch 951/10000, Loss: 1067.8541259765625\n",
      "Training R2 -0.2879678810430366\n",
      "Validation Loss: 830.372802734375\n",
      "Epoch 1001/10000, Loss: 987.8998413085938\n",
      "Training R2 -0.034060285511876565\n",
      "Validation Loss: 749.4918823242188\n",
      "Epoch 1051/10000, Loss: 921.3316650390625\n",
      "Training R2 0.14690313222204854\n",
      "Validation Loss: 680.9959716796875\n",
      "Epoch 1101/10000, Loss: 866.3250122070312\n",
      "Training R2 0.2776706777725537\n",
      "Validation Loss: 623.4765625\n",
      "Epoch 1151/10000, Loss: 820.9605102539062\n",
      "Training R2 0.3738312201917653\n",
      "Validation Loss: 575.279541015625\n",
      "Epoch 1201/10000, Loss: 783.3109130859375\n",
      "Training R2 0.44594457886123684\n",
      "Validation Loss: 534.6535034179688\n",
      "Epoch 1251/10000, Loss: 751.7057495117188\n",
      "Training R2 0.5012207686556158\n",
      "Validation Loss: 500.05560302734375\n",
      "Epoch 1301/10000, Loss: 724.9550170898438\n",
      "Training R2 0.5441187005051784\n",
      "Validation Loss: 470.7154235839844\n",
      "Epoch 1351/10000, Loss: 701.987060546875\n",
      "Training R2 0.577926569499888\n",
      "Validation Loss: 445.5136413574219\n",
      "Epoch 1401/10000, Loss: 681.9786376953125\n",
      "Training R2 0.6050684837392787\n",
      "Validation Loss: 423.5628356933594\n",
      "Epoch 1451/10000, Loss: 664.3152465820312\n",
      "Training R2 0.6273457212071822\n",
      "Validation Loss: 404.0978698730469\n",
      "Epoch 1501/10000, Loss: 648.442138671875\n",
      "Training R2 0.6457411234962277\n",
      "Validation Loss: 386.7872619628906\n",
      "Epoch 1551/10000, Loss: 633.9434814453125\n",
      "Training R2 0.6612529206435989\n",
      "Validation Loss: 371.0605163574219\n",
      "Epoch 1601/10000, Loss: 620.5393676757812\n",
      "Training R2 0.6746618665073744\n",
      "Validation Loss: 356.8387756347656\n",
      "Epoch 1651/10000, Loss: 607.9397583007812\n",
      "Training R2 0.6863366548393944\n",
      "Validation Loss: 344.11260986328125\n",
      "Epoch 1701/10000, Loss: 596.0245361328125\n",
      "Training R2 0.6967165526818774\n",
      "Validation Loss: 332.4618835449219\n",
      "Epoch 1751/10000, Loss: 584.6646118164062\n",
      "Training R2 0.7060786426634996\n",
      "Validation Loss: 321.6741027832031\n",
      "Epoch 1801/10000, Loss: 573.7540283203125\n",
      "Training R2 0.7146695549591742\n",
      "Validation Loss: 311.507568359375\n",
      "Epoch 1851/10000, Loss: 563.1116943359375\n",
      "Training R2 0.7224562313890135\n",
      "Validation Loss: 302.05133056640625\n",
      "Epoch 1901/10000, Loss: 552.6859741210938\n",
      "Training R2 0.7297432413360864\n",
      "Validation Loss: 293.2179260253906\n",
      "Epoch 1951/10000, Loss: 542.48291015625\n",
      "Training R2 0.7366118510099047\n",
      "Validation Loss: 284.8037109375\n",
      "Epoch 2001/10000, Loss: 532.437744140625\n",
      "Training R2 0.7431085261177968\n",
      "Validation Loss: 276.7427062988281\n",
      "Epoch 2051/10000, Loss: 522.5816040039062\n",
      "Training R2 0.7493070071860632\n",
      "Validation Loss: 269.0923156738281\n",
      "Epoch 2101/10000, Loss: 512.8704833984375\n",
      "Training R2 0.7553085934861692\n",
      "Validation Loss: 261.77020263671875\n",
      "Epoch 2151/10000, Loss: 503.3217468261719\n",
      "Training R2 0.7611163418359902\n",
      "Validation Loss: 254.69984436035156\n",
      "Epoch 2201/10000, Loss: 493.987548828125\n",
      "Training R2 0.7667668243487218\n",
      "Validation Loss: 247.92713928222656\n",
      "Epoch 2251/10000, Loss: 484.82391357421875\n",
      "Training R2 0.7722095404092265\n",
      "Validation Loss: 241.4300994873047\n",
      "Epoch 2301/10000, Loss: 475.8551940917969\n",
      "Training R2 0.7774744445404058\n",
      "Validation Loss: 235.1820526123047\n",
      "Epoch 2351/10000, Loss: 467.0945129394531\n",
      "Training R2 0.7825840243904784\n",
      "Validation Loss: 229.20816040039062\n",
      "Epoch 2401/10000, Loss: 458.5025939941406\n",
      "Training R2 0.787557066452231\n",
      "Validation Loss: 223.5211639404297\n",
      "Epoch 2451/10000, Loss: 450.0921325683594\n",
      "Training R2 0.7923634476941934\n",
      "Validation Loss: 218.08116149902344\n",
      "Epoch 2501/10000, Loss: 441.87109375\n",
      "Training R2 0.7970222407322277\n",
      "Validation Loss: 212.79721069335938\n",
      "Epoch 2551/10000, Loss: 433.8365783691406\n",
      "Training R2 0.8015670698164198\n",
      "Validation Loss: 207.70068359375\n",
      "Epoch 2601/10000, Loss: 425.9526672363281\n",
      "Training R2 0.8059813789542973\n",
      "Validation Loss: 202.8109893798828\n",
      "Epoch 2651/10000, Loss: 418.2246398925781\n",
      "Training R2 0.8102193916731776\n",
      "Validation Loss: 198.15841674804688\n",
      "Epoch 2701/10000, Loss: 410.64849853515625\n",
      "Training R2 0.814380073908376\n",
      "Validation Loss: 193.67544555664062\n",
      "Epoch 2751/10000, Loss: 403.2188720703125\n",
      "Training R2 0.8184296346876104\n",
      "Validation Loss: 189.32406616210938\n",
      "Epoch 2801/10000, Loss: 395.93133544921875\n",
      "Training R2 0.8223369553597034\n",
      "Validation Loss: 185.12606811523438\n",
      "Epoch 2851/10000, Loss: 388.8255310058594\n",
      "Training R2 0.8262152579634948\n",
      "Validation Loss: 181.10247802734375\n",
      "Epoch 2901/10000, Loss: 381.89434814453125\n",
      "Training R2 0.8300116894610923\n",
      "Validation Loss: 177.25352478027344\n",
      "Epoch 2951/10000, Loss: 375.1092224121094\n",
      "Training R2 0.8337047143529805\n",
      "Validation Loss: 173.59027099609375\n",
      "Epoch 3001/10000, Loss: 368.4779052734375\n",
      "Training R2 0.8372684062471387\n",
      "Validation Loss: 170.1392822265625\n",
      "Epoch 3051/10000, Loss: 362.03662109375\n",
      "Training R2 0.8406972003537978\n",
      "Validation Loss: 166.84994506835938\n",
      "Epoch 3101/10000, Loss: 355.7627868652344\n",
      "Training R2 0.844018766335535\n",
      "Validation Loss: 163.78738403320312\n",
      "Epoch 3151/10000, Loss: 349.6648864746094\n",
      "Training R2 0.8472512623191973\n",
      "Validation Loss: 160.90846252441406\n",
      "Epoch 3201/10000, Loss: 343.70947265625\n",
      "Training R2 0.8503480910297563\n",
      "Validation Loss: 158.27020263671875\n",
      "Epoch 3251/10000, Loss: 337.86883544921875\n",
      "Training R2 0.8533963390075789\n",
      "Validation Loss: 155.94358825683594\n",
      "Epoch 3301/10000, Loss: 332.2152404785156\n",
      "Training R2 0.8563869797998781\n",
      "Validation Loss: 153.61325073242188\n",
      "Epoch 3351/10000, Loss: 326.7216796875\n",
      "Training R2 0.8592572207822872\n",
      "Validation Loss: 151.47462463378906\n",
      "Epoch 3401/10000, Loss: 321.37445068359375\n",
      "Training R2 0.8620207960340883\n",
      "Validation Loss: 149.46705627441406\n",
      "Epoch 3451/10000, Loss: 316.16973876953125\n",
      "Training R2 0.8646951492747033\n",
      "Validation Loss: 147.5696258544922\n",
      "Epoch 3501/10000, Loss: 311.1086120605469\n",
      "Training R2 0.8672825907890632\n",
      "Validation Loss: 145.78070068359375\n",
      "Epoch 3551/10000, Loss: 306.1854248046875\n",
      "Training R2 0.8697804175230848\n",
      "Validation Loss: 144.14060974121094\n",
      "Epoch 3601/10000, Loss: 301.3959655761719\n",
      "Training R2 0.8721910433058964\n",
      "Validation Loss: 142.63023376464844\n",
      "Epoch 3651/10000, Loss: 296.7321472167969\n",
      "Training R2 0.8745239448025557\n",
      "Validation Loss: 141.26365661621094\n",
      "Epoch 3701/10000, Loss: 292.1878662109375\n",
      "Training R2 0.8767976984459723\n",
      "Validation Loss: 139.97811889648438\n",
      "Epoch 3751/10000, Loss: 287.7679748535156\n",
      "Training R2 0.8789907166722293\n",
      "Validation Loss: 138.7986297607422\n",
      "Epoch 3801/10000, Loss: 283.46978759765625\n",
      "Training R2 0.8811174338253965\n",
      "Validation Loss: 137.72557067871094\n",
      "Epoch 3851/10000, Loss: 279.2767028808594\n",
      "Training R2 0.8831673998540474\n",
      "Validation Loss: 136.80612182617188\n",
      "Epoch 3901/10000, Loss: 275.18682861328125\n",
      "Training R2 0.8851931341177509\n",
      "Validation Loss: 136.04722595214844\n",
      "Epoch 3951/10000, Loss: 271.2169494628906\n",
      "Training R2 0.887119503034469\n",
      "Validation Loss: 135.39015197753906\n",
      "Epoch 4001/10000, Loss: 267.35528564453125\n",
      "Training R2 0.8889991064418726\n",
      "Validation Loss: 134.87521362304688\n",
      "Epoch 4051/10000, Loss: 263.4946594238281\n",
      "Training R2 0.8907763893980147\n",
      "Validation Loss: 134.43759155273438\n",
      "Epoch 4101/10000, Loss: 259.6354675292969\n",
      "Training R2 0.8926133571557407\n",
      "Validation Loss: 134.09463500976562\n",
      "Epoch 4151/10000, Loss: 255.91268920898438\n",
      "Training R2 0.8944170806718719\n",
      "Validation Loss: 133.90187072753906\n",
      "Epoch 4201/10000, Loss: 252.3090362548828\n",
      "Training R2 0.8961390811374501\n",
      "Validation Loss: 133.73963928222656\n",
      "Epoch 4251/10000, Loss: 248.784912109375\n",
      "Training R2 0.8978170218342847\n",
      "Validation Loss: 133.6435546875\n",
      "Epoch 4301/10000, Loss: 245.3523712158203\n",
      "Training R2 0.8994521384133717\n",
      "Validation Loss: 133.5819091796875\n",
      "Epoch 4351/10000, Loss: 241.95120239257812\n",
      "Training R2 0.9010280385953573\n",
      "Validation Loss: 133.6221466064453\n",
      "Epoch 4401/10000, Loss: 238.55697631835938\n",
      "Training R2 0.9026028501323271\n",
      "Validation Loss: 133.8036346435547\n",
      "Epoch 4451/10000, Loss: 235.2596893310547\n",
      "Training R2 0.9041400315247999\n",
      "Validation Loss: 133.9952392578125\n",
      "Epoch 4501/10000, Loss: 231.79393005371094\n",
      "Training R2 0.905643123620852\n",
      "Validation Loss: 134.15423583984375\n",
      "Epoch 4551/10000, Loss: 228.45721435546875\n",
      "Training R2 0.9072157921090691\n",
      "Validation Loss: 134.39041137695312\n",
      "Epoch 4601/10000, Loss: 225.23336791992188\n",
      "Training R2 0.9087213209734721\n",
      "Validation Loss: 134.7884521484375\n",
      "Epoch 4651/10000, Loss: 222.10018920898438\n",
      "Training R2 0.9101287879213277\n",
      "Validation Loss: 135.22122192382812\n",
      "Epoch 4701/10000, Loss: 219.05133056640625\n",
      "Training R2 0.9115301174466685\n",
      "Validation Loss: 135.6289825439453\n",
      "Epoch 4751/10000, Loss: 216.03797912597656\n",
      "Training R2 0.9129132576460421\n",
      "Validation Loss: 135.94432067871094\n",
      "Epoch 4801/10000, Loss: 213.0732879638672\n",
      "Training R2 0.9142179981952159\n",
      "Validation Loss: 136.2869415283203\n",
      "Epoch 4851/10000, Loss: 210.15164184570312\n",
      "Training R2 0.915515129323973\n",
      "Validation Loss: 136.6898193359375\n",
      "Epoch 4901/10000, Loss: 207.25274658203125\n",
      "Training R2 0.9168052244499235\n",
      "Validation Loss: 137.05130004882812\n",
      "Epoch 4951/10000, Loss: 204.40203857421875\n",
      "Training R2 0.9180673684715273\n",
      "Validation Loss: 137.31787109375\n",
      "Epoch 5001/10000, Loss: 201.598388671875\n",
      "Training R2 0.9193497016818873\n",
      "Validation Loss: 137.4458770751953\n",
      "Epoch 5051/10000, Loss: 198.8354949951172\n",
      "Training R2 0.9205528021782792\n",
      "Validation Loss: 137.5988311767578\n",
      "Epoch 5101/10000, Loss: 196.117919921875\n",
      "Training R2 0.9217515898002688\n",
      "Validation Loss: 137.74302673339844\n",
      "Epoch 5151/10000, Loss: 193.42149353027344\n",
      "Training R2 0.9229335090320063\n",
      "Validation Loss: 137.9140625\n",
      "Epoch 5201/10000, Loss: 190.76214599609375\n",
      "Training R2 0.9240920650127381\n",
      "Validation Loss: 138.11111450195312\n",
      "Epoch 5251/10000, Loss: 188.1512451171875\n",
      "Training R2 0.9252199818663405\n",
      "Validation Loss: 138.29454040527344\n",
      "Epoch 5301/10000, Loss: 185.560791015625\n",
      "Training R2 0.9263457628127514\n",
      "Validation Loss: 138.43939208984375\n",
      "Epoch 5351/10000, Loss: 182.98707580566406\n",
      "Training R2 0.9274675119994196\n",
      "Validation Loss: 138.68173217773438\n",
      "Epoch 5401/10000, Loss: 180.45335388183594\n",
      "Training R2 0.9285331117165702\n",
      "Validation Loss: 138.92662048339844\n",
      "Epoch 5451/10000, Loss: 177.92837524414062\n",
      "Training R2 0.9296160946038503\n",
      "Validation Loss: 139.1371307373047\n",
      "Epoch 5501/10000, Loss: 175.392333984375\n",
      "Training R2 0.9307031629149765\n",
      "Validation Loss: 139.4257354736328\n",
      "Epoch 5551/10000, Loss: 172.86270141601562\n",
      "Training R2 0.9317824698705297\n",
      "Validation Loss: 139.92086791992188\n",
      "Epoch 5601/10000, Loss: 170.36959838867188\n",
      "Training R2 0.9328163113103564\n",
      "Validation Loss: 140.43959045410156\n",
      "Epoch 5651/10000, Loss: 167.93545532226562\n",
      "Training R2 0.9338458325024728\n",
      "Validation Loss: 140.8932647705078\n",
      "Epoch 5701/10000, Loss: 165.55262756347656\n",
      "Training R2 0.9348692725513676\n",
      "Validation Loss: 141.20574951171875\n",
      "Epoch 5751/10000, Loss: 163.19943237304688\n",
      "Training R2 0.935850199561061\n",
      "Validation Loss: 141.6490020751953\n",
      "Epoch 5801/10000, Loss: 160.86341857910156\n",
      "Training R2 0.9368437067567384\n",
      "Validation Loss: 142.0113067626953\n",
      "Epoch 5851/10000, Loss: 158.30105590820312\n",
      "Training R2 0.9378852050987142\n",
      "Validation Loss: 142.47610473632812\n",
      "Epoch 5901/10000, Loss: 155.88626098632812\n",
      "Training R2 0.9389334663746663\n",
      "Validation Loss: 142.66754150390625\n",
      "Epoch 5951/10000, Loss: 153.510498046875\n",
      "Training R2 0.9399176357444234\n",
      "Validation Loss: 142.81398010253906\n",
      "Epoch 6001/10000, Loss: 151.15098571777344\n",
      "Training R2 0.9409056019858164\n",
      "Validation Loss: 142.9401397705078\n",
      "Epoch 6051/10000, Loss: 148.80172729492188\n",
      "Training R2 0.9418805502155874\n",
      "Validation Loss: 142.93991088867188\n",
      "Epoch 6101/10000, Loss: 146.4910888671875\n",
      "Training R2 0.9428291231472385\n",
      "Validation Loss: 142.9994659423828\n",
      "Epoch 6151/10000, Loss: 144.2059326171875\n",
      "Training R2 0.9437753441986992\n",
      "Validation Loss: 143.01206970214844\n",
      "Epoch 6201/10000, Loss: 141.927490234375\n",
      "Training R2 0.9447121940097907\n",
      "Validation Loss: 143.25448608398438\n",
      "Epoch 6251/10000, Loss: 139.70716857910156\n",
      "Training R2 0.9456257864395339\n",
      "Validation Loss: 143.6585693359375\n",
      "Epoch 6301/10000, Loss: 137.52134704589844\n",
      "Training R2 0.9465322531030638\n",
      "Validation Loss: 143.9519500732422\n",
      "Epoch 6351/10000, Loss: 135.35023498535156\n",
      "Training R2 0.9474353705514919\n",
      "Validation Loss: 144.2992401123047\n",
      "Epoch 6401/10000, Loss: 133.18557739257812\n",
      "Training R2 0.948343434801146\n",
      "Validation Loss: 144.66973876953125\n",
      "Epoch 6451/10000, Loss: 131.03369140625\n",
      "Training R2 0.9492178257740465\n",
      "Validation Loss: 144.9306182861328\n",
      "Epoch 6501/10000, Loss: 128.89706420898438\n",
      "Training R2 0.9500950214793257\n",
      "Validation Loss: 145.0557861328125\n",
      "Epoch 6551/10000, Loss: 126.77726745605469\n",
      "Training R2 0.9509465583227835\n",
      "Validation Loss: 145.26739501953125\n",
      "Epoch 6601/10000, Loss: 124.64468383789062\n",
      "Training R2 0.9518059408926262\n",
      "Validation Loss: 145.5505828857422\n",
      "Epoch 6651/10000, Loss: 122.51497650146484\n",
      "Training R2 0.952677983665201\n",
      "Validation Loss: 145.79270935058594\n",
      "Epoch 6701/10000, Loss: 120.41841888427734\n",
      "Training R2 0.953528409759949\n",
      "Validation Loss: 146.0279541015625\n",
      "Epoch 6751/10000, Loss: 118.38412475585938\n",
      "Training R2 0.9543534100168851\n",
      "Validation Loss: 146.0763702392578\n",
      "Epoch 6801/10000, Loss: 116.21002197265625\n",
      "Training R2 0.9552326287676207\n",
      "Validation Loss: 145.72605895996094\n",
      "Epoch 6851/10000, Loss: 114.0395278930664\n",
      "Training R2 0.9560928765847365\n",
      "Validation Loss: 145.37217712402344\n",
      "Epoch 6901/10000, Loss: 111.91554260253906\n",
      "Training R2 0.9569422226143196\n",
      "Validation Loss: 145.2013702392578\n",
      "Epoch 6951/10000, Loss: 109.81466674804688\n",
      "Training R2 0.9577836526706328\n",
      "Validation Loss: 145.13816833496094\n",
      "Epoch 7001/10000, Loss: 107.80158233642578\n",
      "Training R2 0.9586142453585922\n",
      "Validation Loss: 144.87692260742188\n",
      "Epoch 7051/10000, Loss: 105.8442153930664\n",
      "Training R2 0.9594047314173159\n",
      "Validation Loss: 144.62631225585938\n",
      "Epoch 7101/10000, Loss: 103.91497802734375\n",
      "Training R2 0.960185491748621\n",
      "Validation Loss: 144.3687286376953\n",
      "Epoch 7151/10000, Loss: 102.0160903930664\n",
      "Training R2 0.9609487859051374\n",
      "Validation Loss: 144.2967071533203\n",
      "Epoch 7201/10000, Loss: 100.13224029541016\n",
      "Training R2 0.9617068264652672\n",
      "Validation Loss: 144.1697235107422\n",
      "Epoch 7251/10000, Loss: 98.27610778808594\n",
      "Training R2 0.9624465715579354\n",
      "Validation Loss: 144.2492218017578\n",
      "Epoch 7301/10000, Loss: 96.45578002929688\n",
      "Training R2 0.9631757895323759\n",
      "Validation Loss: 144.4745330810547\n",
      "Epoch 7351/10000, Loss: 94.65975189208984\n",
      "Training R2 0.9638978425255382\n",
      "Validation Loss: 144.68727111816406\n",
      "Epoch 7401/10000, Loss: 92.8726577758789\n",
      "Training R2 0.9646117224850099\n",
      "Validation Loss: 144.95840454101562\n",
      "Epoch 7451/10000, Loss: 91.10652160644531\n",
      "Training R2 0.9653149292726001\n",
      "Validation Loss: 145.23147583007812\n",
      "Epoch 7501/10000, Loss: 89.34195709228516\n",
      "Training R2 0.9660129749369398\n",
      "Validation Loss: 145.3411407470703\n",
      "Epoch 7551/10000, Loss: 87.5193862915039\n",
      "Training R2 0.9667404229431562\n",
      "Validation Loss: 145.53831481933594\n",
      "Epoch 7601/10000, Loss: 85.78814697265625\n",
      "Training R2 0.9674229787501462\n",
      "Validation Loss: 145.76991271972656\n",
      "Epoch 7651/10000, Loss: 84.08245086669922\n",
      "Training R2 0.9680977839777805\n",
      "Validation Loss: 145.94876098632812\n",
      "Epoch 7701/10000, Loss: 82.39891052246094\n",
      "Training R2 0.9687609549354961\n",
      "Validation Loss: 146.12477111816406\n",
      "Epoch 7751/10000, Loss: 80.76469421386719\n",
      "Training R2 0.969406623499696\n",
      "Validation Loss: 146.26959228515625\n",
      "Epoch 7801/10000, Loss: 79.1436996459961\n",
      "Training R2 0.9700359991060927\n",
      "Validation Loss: 146.58946228027344\n",
      "Epoch 7851/10000, Loss: 77.56002807617188\n",
      "Training R2 0.9706616958080516\n",
      "Validation Loss: 146.89659118652344\n",
      "Epoch 7901/10000, Loss: 76.0173110961914\n",
      "Training R2 0.9712660874267587\n",
      "Validation Loss: 147.1178436279297\n",
      "Epoch 7951/10000, Loss: 74.48365020751953\n",
      "Training R2 0.9718664943545349\n",
      "Validation Loss: 147.35342407226562\n",
      "Epoch 8001/10000, Loss: 72.96421813964844\n",
      "Training R2 0.9724590420331162\n",
      "Validation Loss: 147.64407348632812\n",
      "Epoch 8051/10000, Loss: 71.4849853515625\n",
      "Training R2 0.9730396097594134\n",
      "Validation Loss: 147.85581970214844\n",
      "Epoch 8101/10000, Loss: 70.04170989990234\n",
      "Training R2 0.9736077664690832\n",
      "Validation Loss: 148.08180236816406\n",
      "Epoch 8151/10000, Loss: 68.62142944335938\n",
      "Training R2 0.9741623614172644\n",
      "Validation Loss: 148.28216552734375\n",
      "Epoch 8201/10000, Loss: 67.22270965576172\n",
      "Training R2 0.9747081642194587\n",
      "Validation Loss: 148.5343780517578\n",
      "Epoch 8251/10000, Loss: 65.83860778808594\n",
      "Training R2 0.9752449874958746\n",
      "Validation Loss: 148.80661010742188\n",
      "Epoch 8301/10000, Loss: 64.48099517822266\n",
      "Training R2 0.9757716286114214\n",
      "Validation Loss: 149.09414672851562\n",
      "Epoch 8351/10000, Loss: 63.16633224487305\n",
      "Training R2 0.9762834449945946\n",
      "Validation Loss: 149.38258361816406\n",
      "Epoch 8401/10000, Loss: 61.878692626953125\n",
      "Training R2 0.9767829305345802\n",
      "Validation Loss: 149.7016143798828\n",
      "Epoch 8451/10000, Loss: 60.61570739746094\n",
      "Training R2 0.9772716011369255\n",
      "Validation Loss: 149.97824096679688\n",
      "Epoch 8501/10000, Loss: 59.3702278137207\n",
      "Training R2 0.9777556272364375\n",
      "Validation Loss: 150.31057739257812\n",
      "Epoch 8551/10000, Loss: 58.15296936035156\n",
      "Training R2 0.9782261189488772\n",
      "Validation Loss: 150.66458129882812\n",
      "Epoch 8601/10000, Loss: 56.96147918701172\n",
      "Training R2 0.9786877415423826\n",
      "Validation Loss: 151.10226440429688\n",
      "Epoch 8651/10000, Loss: 55.80338668823242\n",
      "Training R2 0.97913921755576\n",
      "Validation Loss: 151.49398803710938\n",
      "Epoch 8701/10000, Loss: 54.65947341918945\n",
      "Training R2 0.9795821812886746\n",
      "Validation Loss: 151.8637237548828\n",
      "Epoch 8751/10000, Loss: 53.54252243041992\n",
      "Training R2 0.980012989448265\n",
      "Validation Loss: 152.16278076171875\n",
      "Epoch 8801/10000, Loss: 52.432594299316406\n",
      "Training R2 0.9804400305680782\n",
      "Validation Loss: 152.5542449951172\n",
      "Epoch 8851/10000, Loss: 51.286354064941406\n",
      "Training R2 0.9808770532637086\n",
      "Validation Loss: 153.0838165283203\n",
      "Epoch 8901/10000, Loss: 50.144256591796875\n",
      "Training R2 0.981312114480401\n",
      "Validation Loss: 153.7378387451172\n",
      "Epoch 8951/10000, Loss: 49.084022521972656\n",
      "Training R2 0.9817222302840931\n",
      "Validation Loss: 154.44149780273438\n",
      "Epoch 9001/10000, Loss: 48.02332305908203\n",
      "Training R2 0.9821285995973558\n",
      "Validation Loss: 154.96170043945312\n",
      "Epoch 9051/10000, Loss: 46.974266052246094\n",
      "Training R2 0.9825307091687716\n",
      "Validation Loss: 155.46495056152344\n",
      "Epoch 9101/10000, Loss: 45.98225021362305\n",
      "Training R2 0.9829150414231893\n",
      "Validation Loss: 156.10801696777344\n",
      "Epoch 9151/10000, Loss: 45.01533508300781\n",
      "Training R2 0.9832873395623809\n",
      "Validation Loss: 156.5983428955078\n",
      "Epoch 9201/10000, Loss: 44.07588195800781\n",
      "Training R2 0.9836476615857705\n",
      "Validation Loss: 157.04043579101562\n",
      "Epoch 9251/10000, Loss: 43.16221618652344\n",
      "Training R2 0.9839968767745714\n",
      "Validation Loss: 157.4203643798828\n",
      "Epoch 9301/10000, Loss: 42.24650955200195\n",
      "Training R2 0.9843505851370544\n",
      "Validation Loss: 157.78878784179688\n",
      "Epoch 9351/10000, Loss: 41.310272216796875\n",
      "Training R2 0.9847012553647256\n",
      "Validation Loss: 157.78390502929688\n",
      "Epoch 9401/10000, Loss: 40.42653274536133\n",
      "Training R2 0.9850402138693991\n",
      "Validation Loss: 158.2540740966797\n",
      "Epoch 9451/10000, Loss: 39.57283401489258\n",
      "Training R2 0.9853655269682405\n",
      "Validation Loss: 158.63108825683594\n",
      "Epoch 9501/10000, Loss: 38.75934982299805\n",
      "Training R2 0.9856778425543523\n",
      "Validation Loss: 158.91099548339844\n",
      "Epoch 9551/10000, Loss: 37.970428466796875\n",
      "Training R2 0.9859807648108749\n",
      "Validation Loss: 159.0447540283203\n",
      "Epoch 9601/10000, Loss: 37.1392936706543\n",
      "Training R2 0.9862935034028144\n",
      "Validation Loss: 158.42568969726562\n",
      "Epoch 9651/10000, Loss: 36.35029602050781\n",
      "Training R2 0.9865880817559534\n",
      "Validation Loss: 157.86151123046875\n",
      "Epoch 9701/10000, Loss: 35.59198760986328\n",
      "Training R2 0.9868751642311773\n",
      "Validation Loss: 157.85646057128906\n",
      "Epoch 9751/10000, Loss: 34.852256774902344\n",
      "Training R2 0.987155474834228\n",
      "Validation Loss: 157.78953552246094\n",
      "Epoch 9801/10000, Loss: 34.13041687011719\n",
      "Training R2 0.987428817513589\n",
      "Validation Loss: 157.57208251953125\n",
      "Epoch 9851/10000, Loss: 33.4248161315918\n",
      "Training R2 0.9876950505572485\n",
      "Validation Loss: 157.41050720214844\n",
      "Epoch 9901/10000, Loss: 32.73347473144531\n",
      "Training R2 0.987955523888902\n",
      "Validation Loss: 157.34426879882812\n",
      "Epoch 9951/10000, Loss: 32.0510368347168\n",
      "Training R2 0.9882129428251252\n",
      "Validation Loss: 157.1009063720703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# For each epoch...\n",
    "trainloss_vs_epochs = []\n",
    "valloss_vs_epochs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    inputs = torch.from_numpy(X_train).float()\n",
    "    targets = torch.from_numpy(y_train).view(-1, 1).float()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # Track training loss\n",
    "        trainloss_vs_epochs.append([epoch,math.sqrt(loss.detach().item())])\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}\")\n",
    "        print('Training R2', r2_score(outputs.detach().numpy(), targets.detach().numpy()))\n",
    "\n",
    "    # Validation\n",
    "    if epoch % 50 == 0:\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        val_inputs = torch.from_numpy(X_val).float()\n",
    "        val_targets = torch.from_numpy(y_val).view(-1, 1).float()\n",
    "\n",
    "        val_outputs = net(val_inputs)\n",
    "        val_loss = loss_fn(val_outputs, val_targets)\n",
    "        valloss_vs_epochs.append([epoch,math.sqrt(val_loss.item())])\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss.detach().item()}\")\n",
    "# Evaluate on the test set\n",
    "net.eval()\n",
    "test_inputs = torch.from_numpy(X_test).float()\n",
    "test_targets = torch.from_numpy(y_test).view(-1, 1).float()\n",
    "\n",
    "test_outputs = net(test_inputs)\n",
    "test_loss = loss_fn(test_outputs, test_targets)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 166.6793212890625\n",
      "Test R2 0.9451977300761792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Test Loss: {test_loss.detach().item()}\")\n",
    "print('Test R2', r2_score(test_outputs.detach().numpy(), test_targets.detach().numpy()))\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), save_dir + 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label data and save in specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outputs_labelled = np.column_stack((val_outputs.detach().numpy(),y_val,label_val))\n",
    "test_outputs_labelled = np.column_stack((test_outputs.detach().numpy(),y_test,label_test))\n",
    "train_outputs_labelled = np.column_stack((outputs.detach().numpy(),y_train,label_train))\n",
    "\n",
    "np.savetxt(save_dir+'trainloss.csv',trainloss_vs_epochs,delimiter=',')\n",
    "np.savetxt(save_dir+'valloss.csv',valloss_vs_epochs,delimiter=',')\n",
    "np.savetxt(save_dir+'testpred.csv', test_outputs_labelled,delimiter=',')\n",
    "np.savetxt(save_dir+'trainpred.csv', train_outputs_labelled,delimiter=',')\n",
    "np.savetxt(save_dir+'valpred.csv', val_outputs_labelled,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
