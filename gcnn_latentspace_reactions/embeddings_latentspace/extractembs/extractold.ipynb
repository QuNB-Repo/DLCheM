{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amerelsamman/anaconda3/envs/dlchem/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['representation.embedding.weight', 'representation.distance_expansion.width', 'representation.distance_expansion.offsets', 'representation.interactions.0.filter_network.0.weight', 'representation.interactions.0.filter_network.0.bias', 'representation.interactions.0.filter_network.1.weight', 'representation.interactions.0.filter_network.1.bias', 'representation.interactions.0.cutoff_network.cutoff', 'representation.interactions.0.cfconv.in2f.weight', 'representation.interactions.0.cfconv.f2out.weight', 'representation.interactions.0.cfconv.f2out.bias', 'representation.interactions.0.cfconv.filter_network.0.weight', 'representation.interactions.0.cfconv.filter_network.0.bias', 'representation.interactions.0.cfconv.filter_network.1.weight', 'representation.interactions.0.cfconv.filter_network.1.bias', 'representation.interactions.0.cfconv.cutoff_network.cutoff', 'representation.interactions.0.dense.weight', 'representation.interactions.0.dense.bias', 'representation.interactions.1.filter_network.0.weight', 'representation.interactions.1.filter_network.0.bias', 'representation.interactions.1.filter_network.1.weight', 'representation.interactions.1.filter_network.1.bias', 'representation.interactions.1.cutoff_network.cutoff', 'representation.interactions.1.cfconv.in2f.weight', 'representation.interactions.1.cfconv.f2out.weight', 'representation.interactions.1.cfconv.f2out.bias', 'representation.interactions.1.cfconv.filter_network.0.weight', 'representation.interactions.1.cfconv.filter_network.0.bias', 'representation.interactions.1.cfconv.filter_network.1.weight', 'representation.interactions.1.cfconv.filter_network.1.bias', 'representation.interactions.1.cfconv.cutoff_network.cutoff', 'representation.interactions.1.dense.weight', 'representation.interactions.1.dense.bias', 'representation.interactions.2.filter_network.0.weight', 'representation.interactions.2.filter_network.0.bias', 'representation.interactions.2.filter_network.1.weight', 'representation.interactions.2.filter_network.1.bias', 'representation.interactions.2.cutoff_network.cutoff', 'representation.interactions.2.cfconv.in2f.weight', 'representation.interactions.2.cfconv.f2out.weight', 'representation.interactions.2.cfconv.f2out.bias', 'representation.interactions.2.cfconv.filter_network.0.weight', 'representation.interactions.2.cfconv.filter_network.0.bias', 'representation.interactions.2.cfconv.filter_network.1.weight', 'representation.interactions.2.cfconv.filter_network.1.bias', 'representation.interactions.2.cfconv.cutoff_network.cutoff', 'representation.interactions.2.dense.weight', 'representation.interactions.2.dense.bias', 'output_modules.0.atomref.weight', 'output_modules.0.out_net.1.out_net.0.weight', 'output_modules.0.out_net.1.out_net.0.bias', 'output_modules.0.out_net.1.out_net.1.weight', 'output_modules.0.out_net.1.out_net.1.bias', 'output_modules.0.standardize.mean', 'output_modules.0.standardize.stddev'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "from schnetpack import AtomsData\n",
    "import numpy as np\n",
    "\n",
    "def hook(self, inp_tensor, out_tensor):\n",
    "    # Self is included and refers to the model class\n",
    "    # Global allows us to utilize embedding_output outside the current function scope\n",
    "    global layer\n",
    "    #Update the embedding_output variable to be equal to our output tensor\n",
    "    layer=out_tensor \n",
    "\n",
    "\n",
    "n_atom_basis = 128\n",
    "n_filters = 128\n",
    "n_gaussians = 50\n",
    "n_interactions = 6 \n",
    "cutoff = 50. \n",
    "\n",
    "\n",
    "qm9_filepath = 'data/datasets/QM9/qm9.db'\n",
    "qm9_data = QM9(qm9_filepath,download=False,remove_uncharacterized=True)\n",
    "\n",
    "data_filepath = 'data/datasets/QM9/qm9.db'\n",
    "available_properties = 'N/A'\n",
    "data = QM9(qm9_filepath,download=False,remove_uncharacterized=True)\n",
    "\n",
    "\n",
    "# Load atom ref data \n",
    "atomrefs = qm9_data.get_atomref(QM9.U0)\n",
    "\n",
    "# Define SchNet representation model\n",
    "\n",
    "schnet = spk.representation.SchNet(\n",
    "n_atom_basis=n_atom_basis, n_filters=n_filters, n_gaussians=n_gaussians, n_interactions=n_interactions,\n",
    "cutoff=cutoff , cutoff_network=spk.nn.cutoff.CosineCutoff\n",
    ")\n",
    "\n",
    "# Define SchNet output model and property to be predicted\n",
    "output_U0 = spk.atomistic.Atomwise(n_in=n_filters,atomref=atomrefs[QM9.U0])\n",
    "\n",
    "# Define atomistic model\n",
    "model = spk.AtomisticModel(representation=schnet,output_modules=output_U0)\n",
    "\n",
    "\n",
    "# Load saved checkpoint file\n",
    "checkpoint_path = 'data/trainedmodels/model1/trained-1000.pth'\n",
    "load_checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "#qm9_i6_30f_20g-1000-500-4_300.pth\n",
    "# load model's state dictionary from saved checkpoint\n",
    "model.load_state_dict(load_checkpoint)\n",
    "\n",
    "print(model.state_dict().keys())\n",
    "\n",
    "#set up device for forward pass\n",
    "device='cpu'\n",
    "\n",
    "# load atoms converter \n",
    "converter = spk.data.AtomsConverter(device=device)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rep1-rep3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run an input and extract an internal vector\n",
    "total_embH_list = np.zeros(128)\n",
    "total_embC_list = np.zeros(128)\n",
    "total_embN_list = np.zeros(128)\n",
    "total_embO_list = np.zeros(128)\n",
    "\n",
    "int0H_list = np.zeros(128)\n",
    "int1H_list = np.zeros(128)\n",
    "int2H_list = np.zeros(128)\n",
    "int3H_list = np.zeros(128)\n",
    "int4H_list = np.zeros(128)\n",
    "int5H_list = np.zeros(128)\n",
    "int0C_list = np.zeros(128)\n",
    "int1C_list = np.zeros(128)\n",
    "int2C_list = np.zeros(128)\n",
    "int3C_list = np.zeros(128)\n",
    "int4C_list = np.zeros(128)\n",
    "int5C_list = np.zeros(128)\n",
    "int0N_list = np.zeros(128)\n",
    "int1N_list = np.zeros(128)\n",
    "int2N_list = np.zeros(128)\n",
    "int3N_list = np.zeros(128)\n",
    "int4N_list = np.zeros(128)\n",
    "int5N_list = np.zeros(128)\n",
    "int0O_list = np.zeros(128)\n",
    "int1O_list = np.zeros(128)\n",
    "int2O_list = np.zeros(128)\n",
    "int3O_list = np.zeros(128)\n",
    "int4O_list = np.zeros(128)\n",
    "int5O_list = np.zeros(128)\n",
    "\n",
    "for idx in range(10000):\n",
    "    at, props = data.get_properties(idx)\n",
    "\n",
    "\n",
    "    inputs = converter(at)\n",
    "\n",
    "    layer = None \n",
    "    model.representation.embedding.register_forward_hook(hook)\n",
    "    model(inputs)\n",
    "    emb = layer.clone()\n",
    "    emb = layer.detach().numpy()\n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[0].register_forward_hook(hook)   \n",
    "    model(inputs)\n",
    "    int0 = layer.clone()\n",
    "    int0 = int0.detach().numpy()   \n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[1].register_forward_hook(hook)  \n",
    "    model(inputs)\n",
    "    int1 = layer.clone()\n",
    "    int1 = int1.detach().numpy()   \n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[1].register_forward_hook(hook)  \n",
    "    model(inputs)\n",
    "    int2 = layer.clone()\n",
    "    int2 = int2.detach().numpy()   \n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[1].register_forward_hook(hook)  \n",
    "    model(inputs)\n",
    "    int3 = layer.clone()\n",
    "    int3 = int3.detach().numpy()   \n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[1].register_forward_hook(hook)  \n",
    "    model(inputs)\n",
    "    int4 = layer.clone()\n",
    "    int4 = int4.detach().numpy()   \n",
    "\n",
    "    layer = None\n",
    "    model.representation.interactions[1].register_forward_hook(hook)  \n",
    "    model(inputs)\n",
    "    int5 = layer.clone()\n",
    "    int5 = int5.detach().numpy()   \n",
    "\n",
    "\n",
    "    total_emb = emb + int0 + int1 +int2\n",
    "\n",
    "    number_atoms = len(props['_atomic_numbers'])\n",
    "    for each_atom in range(number_atoms):\n",
    "        if props['_atomic_numbers'][each_atom] == 1:\n",
    "            int0_H = np.vstack((int0H_list,int0[0][each_atom]))\n",
    "            int1_H = np.vstack((int1H_list,int1[0][each_atom]))\n",
    "            int2_H = np.vstack((int2H_list,int2[0][each_atom]))\n",
    "            int3_H = np.vstack((int3H_list,int3[0][each_atom]))\n",
    "            int4_H = np.vstack((int4H_list,int4[0][each_atom]))\n",
    "            int5_H = np.vstack((int5H_list,int5[0][each_atom]))\n",
    "\n",
    "            total_embH_list = np.vstack((total_embH_list,total_emb[0][each_atom]))\n",
    "        if props['_atomic_numbers'][each_atom] == 6:\n",
    "            int0_C = np.vstack((int0C_list,int0[0][each_atom]))\n",
    "            int1_C = np.vstack((int1C_list,int1[0][each_atom]))\n",
    "            int2_C = np.vstack((int2C_list,int2[0][each_atom]))\n",
    "            int3_C = np.vstack((int3C_list,int3[0][each_atom]))\n",
    "            int4_C = np.vstack((int4C_list,int4[0][each_atom]))\n",
    "            int5_C = np.vstack((int5C_list,int5[0][each_atom]))\n",
    "\n",
    "            total_embC_list = np.vstack((total_embC_list,total_emb[0][each_atom]))\n",
    "        if props['_atomic_numbers'][each_atom] == 7:\n",
    "            int0_N = np.vstack((int0N_list,int0[0][each_atom]))\n",
    "            int1_N = np.vstack((int1N_list,int1[0][each_atom]))\n",
    "            int2_N = np.vstack((int2N_list,int2[0][each_atom]))\n",
    "            int3_N = np.vstack((int3N_list,int3[0][each_atom]))\n",
    "            int4_N = np.vstack((int4N_list,int4[0][each_atom]))\n",
    "            int5_N = np.vstack((int5N_list,int5[0][each_atom]))\n",
    "\n",
    "            total_embN_list = np.vstack((total_embN_list,total_emb[0][each_atom]))\n",
    "        if props['_atomic_numbers'][each_atom] == 8:\n",
    "            int0_O = np.vstack((int0O_list,int0[0][each_atom]))\n",
    "            int1_O = np.vstack((int1O_list,int1[0][each_atom]))\n",
    "            int2_O = np.vstack((int2O_list,int2[0][each_atom]))\n",
    "            int3_O = np.vstack((int3O_list,int3[0][each_atom]))\n",
    "            int4_O = np.vstack((int4O_list,int4[0][each_atom]))\n",
    "            int5_O = np.vstack((int5O_list,int5[0][each_atom]))\n",
    "\n",
    "            total_embO_list = np.vstack((total_embO_list,total_emb[0][each_atom]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.12359047e+00  5.49322844e-01  1.37732434e+00 -2.94602841e-01\n",
      "  -1.76522803e+00 -7.11925626e-02 -4.49874461e-01  8.53084445e-01\n",
      "  -2.89131194e-01  1.83485579e+00 -7.41182923e-01  3.43575448e-01\n",
      "   3.78622353e-01 -1.03042805e+00  7.60020196e-01 -7.54284620e-01\n",
      "  -3.33554506e-01  3.90934134e+00  2.03850460e+00 -3.68002009e+00\n",
      "   7.76915312e-01 -3.98384547e+00  2.55580902e+00 -3.30014855e-01\n",
      "  -2.76858640e+00 -2.70543885e+00 -2.88286352e+00 -4.28451681e+00\n",
      "  -7.35593855e-01 -1.24175854e-01]\n",
      " [-2.12359047e+00  5.49323201e-01  1.37732387e+00 -2.94602692e-01\n",
      "  -1.76522791e+00 -7.11928010e-02 -4.49874401e-01  8.53084207e-01\n",
      "  -2.89131105e-01  1.83485615e+00 -7.41182804e-01  3.43576044e-01\n",
      "   3.78622234e-01 -1.03042781e+00  7.60020614e-01 -7.54284620e-01\n",
      "  -3.33554745e-01  3.90934110e+00  2.03850460e+00 -3.68001914e+00\n",
      "   7.76915312e-01 -3.98384523e+00  2.55580854e+00 -3.30014795e-01\n",
      "  -2.76858640e+00 -2.70543838e+00 -2.88286352e+00 -4.28451729e+00\n",
      "  -7.35594392e-01 -1.24176033e-01]\n",
      " [-1.84547031e+00  3.67932588e-01  5.01475811e+00  2.72503996e+00\n",
      "  -3.59518361e+00 -3.00444412e+00  1.63477826e+00  2.30086684e+00\n",
      "   1.55180132e+00  4.70987225e+00 -2.22854352e+00 -2.88417959e+00\n",
      "  -1.97178081e-01 -2.59519410e+00  4.45981693e+00 -3.83502960e+00\n",
      "  -2.45933318e+00  6.03706551e+00  2.62655187e+00 -6.23233175e+00\n",
      "   4.58171129e+00 -6.76659441e+00  3.95086360e+00 -3.13710499e+00\n",
      "  -5.10616112e+00 -6.49111414e+00 -5.34950209e+00 -7.08006287e+00\n",
      "  -1.26438963e+00 -2.82584000e+00]\n",
      " [ 6.22705698e+00 -3.00506783e+00  4.61209249e+00  2.84198195e-01\n",
      "  -1.29731810e+00 -3.10708427e+00  3.25327492e+00 -1.46182847e+00\n",
      "   1.47328877e+00  6.37628555e+00 -3.02414560e+00 -4.46115541e+00\n",
      "  -8.82990718e-01  2.43544316e+00  1.13025131e+01 -2.12675762e+00\n",
      "  -5.17692709e+00  9.57060432e+00 -6.50384843e-01 -7.63453913e+00\n",
      "   7.70973063e+00 -3.26437116e+00  3.37385225e+00 -5.66477728e+00\n",
      "  -2.57770061e+00 -5.72423792e+00 -7.05557775e+00 -5.94044209e+00\n",
      "  -3.35166740e+00 -7.27164602e+00]\n",
      " [ 9.93747807e+00 -4.59030628e+00  4.36637020e+00 -3.13877106e-01\n",
      "  -5.41006103e-02 -2.07177186e+00  1.17257905e+00 -6.82641792e+00\n",
      "  -2.07168055e+00  4.95988560e+00 -1.94365132e+00 -1.01874888e-01\n",
      "   3.52755117e+00  3.86762857e+00  1.03484039e+01 -3.32795352e-01\n",
      "  -3.72963452e+00  1.36950588e+01 -3.63360572e+00 -8.30657578e+00\n",
      "   7.91433573e+00 -3.05319858e+00  1.58858955e+00  2.37870157e-01\n",
      "   1.15416312e+00 -2.35635948e+00 -8.19913101e+00 -6.54196930e+00\n",
      "  -6.50687170e+00 -4.90498686e+00]\n",
      " [ 9.93750191e+00 -4.59030247e+00  4.36634493e+00 -3.13919753e-01\n",
      "  -5.40985540e-02 -2.07170510e+00  1.17257738e+00 -6.82638454e+00\n",
      "  -2.07162499e+00  4.95989513e+00 -1.94373930e+00 -1.01818018e-01\n",
      "   3.52757096e+00  3.86772799e+00  1.03483496e+01 -3.32719594e-01\n",
      "  -3.72961855e+00  1.36950369e+01 -3.63353992e+00 -8.30654144e+00\n",
      "   7.91427708e+00 -3.05321550e+00  1.58858228e+00  2.37874448e-01\n",
      "   1.15417111e+00 -2.35632086e+00 -8.19907761e+00 -6.54189205e+00\n",
      "  -6.50681305e+00 -4.90495872e+00]\n",
      " [ 9.76106930e+00 -5.83694887e+00  6.05643654e+00  1.41801476e+00\n",
      "   8.91034245e-01 -4.67240143e+00  1.04710245e+00 -9.69907188e+00\n",
      "  -3.13273716e+00  6.30199814e+00  1.73470461e+00 -3.15549552e-01\n",
      "   9.60487306e-01  2.14379835e+00  1.31213236e+01 -2.57990885e+00\n",
      "  -3.56596088e+00  1.41655979e+01 -4.58078766e+00 -1.06663179e+01\n",
      "   9.93396568e+00 -4.56964111e+00  1.11858475e+00 -1.10717022e+00\n",
      "   7.42700636e-01 -3.25152373e+00 -1.23116636e+01 -9.81428909e+00\n",
      "  -7.38553286e+00 -6.32791281e+00]\n",
      " [ 7.95894575e+00 -1.50225425e+00  4.04455853e+00 -1.84749544e-01\n",
      "  -9.53990817e-01 -9.74753976e-01  2.61295485e+00 -3.54664087e+00\n",
      "  -4.08883393e-01  3.60441399e+00 -2.20161986e+00 -9.79307652e-01\n",
      "   3.56658816e+00  2.44267178e+00  8.11353493e+00 -3.25795025e-01\n",
      "  -3.86844993e+00  1.16095505e+01 -3.09541202e+00 -6.86765862e+00\n",
      "   6.60766268e+00 -2.21152902e+00  1.93421686e+00  9.59423482e-02\n",
      "  -5.60733974e-01 -3.24906492e+00 -5.53032494e+00 -4.60321474e+00\n",
      "  -4.82418299e+00 -5.31157494e+00]\n",
      " [-7.83087134e-01  1.44063854e+00 -6.21151268e-01 -8.45030010e-01\n",
      "  -1.50819016e+00 -2.39763156e-01 -1.05348563e+00  9.62858379e-01\n",
      "  -9.74247694e-01 -1.12799978e+00 -2.71525550e+00  6.64129078e-01\n",
      "   3.72801352e+00  6.27240539e-01 -1.64026821e+00  3.18807542e-01\n",
      "  -2.02139884e-01  3.69209439e-01  4.77942848e+00 -2.06967163e+00\n",
      "  -9.09615397e-01  7.89658606e-01  2.11191845e+00  1.30548763e+00\n",
      "  -1.35512888e+00 -8.34392086e-02  2.77266169e+00 -1.69456637e+00\n",
      "  -9.38920259e-01  3.29926133e+00]\n",
      " [-1.66775596e+00  5.04059136e-01  2.13976622e+00  1.02662516e+00\n",
      "  -2.03247809e+00 -8.21066797e-01 -2.45180294e-01  8.24086308e-01\n",
      "   1.44058168e-02  1.49887371e+00 -1.45262301e+00  2.15303421e+00\n",
      "   2.61603880e+00 -9.71519530e-01 -4.29621756e-01 -4.95721608e-01\n",
      "  -7.83841372e-01  3.97194624e+00  2.12491894e+00 -3.74781609e+00\n",
      "   8.79964828e-01 -6.04328918e+00  2.62512398e+00  1.73066044e+00\n",
      "  -2.93427515e+00 -2.42740011e+00 -2.60137439e+00 -5.07080936e+00\n",
      "  -1.26116782e-01  1.39364707e+00]\n",
      " [ 7.07626104e+00 -1.40745449e+00  3.51454639e+00 -1.11086285e+00\n",
      "  -1.96954989e+00 -7.86452293e-02  2.26133180e+00 -3.90991378e+00\n",
      "  -7.74553239e-01  3.54632211e+00 -1.67042422e+00 -4.09781933e-07\n",
      "   3.19692516e+00  2.92169762e+00  7.65409231e+00  1.41794130e-01\n",
      "  -3.74146509e+00  1.10538206e+01 -2.04095745e+00 -6.30485058e+00\n",
      "   5.93683100e+00 -1.70784271e+00  1.30964005e+00  1.03924446e-01\n",
      "  -1.11922050e+00 -2.40881801e+00 -4.78830671e+00 -4.37660074e+00\n",
      "  -4.58697319e+00 -4.89524651e+00]\n",
      " [-9.44047093e-01  1.29196668e+00  3.02799511e+00  2.11136198e+00\n",
      "  -3.01906466e+00 -1.53213024e+00  1.61473036e+00  2.34084034e+00\n",
      "   8.19440126e-01  1.77255595e+00 -4.10014820e+00 -2.42330551e+00\n",
      "   2.51085186e+00 -7.76835501e-01  1.57286024e+00 -3.38385701e+00\n",
      "  -9.53068912e-01  2.07511044e+00  5.29735088e+00 -2.98031712e+00\n",
      "   2.58221078e+00 -2.87416220e+00  2.51509547e+00 -1.96052158e+00\n",
      "  -2.49822545e+00 -4.19449186e+00 -1.61793217e-01 -3.62135267e+00\n",
      "  -1.53076506e+00  2.93292195e-01]]\n"
     ]
    }
   ],
   "source": [
    "total_embH_list = np.delete(total_embH_list,0,axis=0) \n",
    "total_embC_list = np.delete(total_embC_list,0,axis=0) \n",
    "total_embN_list = np.delete(total_embN_list,0,axis=0) \n",
    "total_embO_list = np.delete(total_embO_list,0,axis=0) \n",
    "int0_H = np.delete(int0H_list,0,axis=0) \n",
    "int1_H = np.delete(int0H_list,0,axis=0) \n",
    "int2_H = np.delete(int0H_list,0,axis=0) \n",
    "int3_H = np.delete(int0H_list,0,axis=0) \n",
    "int4_H = np.delete(int0H_list,0,axis=0) \n",
    "int5_H = np.delete(int0H_list,0,axis=0) \n",
    "int0_C = np.delete(int0C_list,0,axis=0) \n",
    "int1_C = np.delete(int0C_list,0,axis=0) \n",
    "int2_C = np.delete(int0C_list,0,axis=0) \n",
    "int3_C = np.delete(int0C_list,0,axis=0) \n",
    "int4_C = np.delete(int0C_list,0,axis=0) \n",
    "int5_C = np.delete(int0C_list,0,axis=0) \n",
    "int0_N = np.delete(int0N_list,0,axis=0) \n",
    "int1_N = np.delete(int0N_list,0,axis=0) \n",
    "int2_N = np.delete(int0N_list,0,axis=0) \n",
    "int3_N = np.delete(int0N_list,0,axis=0) \n",
    "int4_N = np.delete(int0N_list,0,axis=0) \n",
    "int5_N = np.delete(int0N_list,0,axis=0)\n",
    "int0_O = np.delete(int0O_list,0,axis=0) \n",
    "int1_O = np.delete(int0O_list,0,axis=0) \n",
    "int2_O = np.delete(int0O_list,0,axis=0) \n",
    "int3_O = np.delete(int0O_list,0,axis=0) \n",
    "int4_O = np.delete(int0O_list,0,axis=0) \n",
    "int5_O = np.delete(int0O_list,0,axis=0)  \n",
    "\n",
    "\n",
    "from numpy import savetxt\n",
    "\n",
    "savetxt('embsH.csv',total_embH_list,delimiter=',')\n",
    "savetxt('embsC.csv',total_embC_list,delimiter=',')\n",
    "savetxt('embsN.csv',total_embN_list,delimiter=',')\n",
    "savetxt('embsO.csv',total_embO_list,delimiter=',')\n",
    "savetxt('int0H.csv',int0H_list,delimiter=',')\n",
    "savetxt('int1H.csv',int1H_list,delimiter=',')\n",
    "savetxt('int2H.csv',int2H_list,delimiter=',')\n",
    "savetxt('int3H.csv',int3H_list,delimiter=',')\n",
    "savetxt('int4H.csv',int4H_list,delimiter=',')\n",
    "savetxt('int5H.csv',int5H_list,delimiter=',')\n",
    "savetxt('int0C.csv',int0C_list,delimiter=',')\n",
    "savetxt('int1C.csv',int1C_list,delimiter=',')\n",
    "savetxt('int2C.csv',int2C_list,delimiter=',')\n",
    "savetxt('int3C.csv',int3C_list,delimiter=',')\n",
    "savetxt('int4C.csv',int4C_list,delimiter=',')\n",
    "savetxt('int5C.csv',int5C_list,delimiter=',')\n",
    "savetxt('int0N.csv',int0N_list,delimiter=',')\n",
    "savetxt('int1N.csv',int1N_list,delimiter=',')\n",
    "savetxt('int2N.csv',int2N_list,delimiter=',')\n",
    "savetxt('int3N.csv',int3N_list,delimiter=',')\n",
    "savetxt('int4N.csv',int4N_list,delimiter=',')\n",
    "savetxt('int5N.csv',int5N_list,delimiter=',')\n",
    "savetxt('int0O.csv',int0O_list,delimiter=',')\n",
    "savetxt('int1O.csv',int1O_list,delimiter=',')\n",
    "savetxt('int2O.csv',int2O_list,delimiter=',')\n",
    "savetxt('int3O.csv',int3O_list,delimiter=',')\n",
    "savetxt('int4O.csv',int4O_list,delimiter=',')\n",
    "savetxt('int5O.csv',int5O_list,delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools.utils_dimred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_342/2411526128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_dimred\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tools.utils_dimred'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = total_embH_list\n",
    "save_filepath = 'embsHpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = total_embC_list\n",
    "save_filepath = 'embsCpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = total_embN_list\n",
    "save_filepath = 'embsNpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = total_embO_list\n",
    "save_filepath = 'embsOpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int0H_list\n",
    "save_filepath = 'int0Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int1H_list\n",
    "save_filepath = 'int1Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int2H_list\n",
    "save_filepath = 'int2Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int3H_list\n",
    "save_filepath = 'int3Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int4H_list\n",
    "save_filepath = 'int4Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "data = int5H_list\n",
    "save_filepath = 'int5Hpca.csv'\n",
    "\n",
    "#perform PCA decomposition of the data\n",
    "pca = PCA(n_components)\n",
    "pca.fit()\n",
    "x_pca = pca.transform(data)\n",
    "cov = pca.get_covariance()\n",
    "eig, ev = la.eig(cov)\n",
    "total = sum(eig.real)\n",
    "evt=np.transpose(ev)\n",
    "unit=np.matmul(ev,evt)\n",
    "\n",
    "\n",
    "save_filepathpca = save_filepath.replace('.csv','transform.csv')\n",
    "savetxt(save_filepathpca, x_pca, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','eig.csv')\n",
    "eig=eig.real\n",
    "savetxt(save_filepatheig, eig, delimiter=',')\n",
    "save_filepatheig = save_filepath.replace('.csv','ev.csv')\n",
    "ev=ev.real\n",
    "savetxt(save_filepatheig, ev, delimiter=',')\n",
    "save_filepathcov = save_filepath.replace('.csv','cov.csv')\n",
    "cov=cov.real\n",
    "savetxt(save_filepathcov, cov, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-process with pandas and stack the label stuff on all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d8435ccd79177254b883c2dec0668c8ed2f1e4b0134a8204790627afd7a6d86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
